{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngram Inverted Index Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import numpy\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import os\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import import_ipynb\n",
    "import clir_files as files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PICKLE_READ_WRITE = False\n",
    "global PUNCTUATION\n",
    "global NUM_BYTES\n",
    "global BYTE_ORDER\n",
    "global SIGNED \n",
    "global STOP_WORDS\n",
    "global RUN_TESTS\n",
    "RUN_TESTS = True\n",
    "TERM_COUNT = 0\n",
    "SIGNED = False\n",
    "BYTE_ORDER = \"big\"\n",
    "NUM_BYTES = 4\n",
    "\n",
    "PUNCTUATION = set(string.punctuation)\n",
    "\n",
    "EN_STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "NGRAM_LEN = 4\n",
    "\n",
    "def increment_term_count():\n",
    "    global TERM_COUNT\n",
    "    TERM_COUNT = TERM_COUNT + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus():\n",
    "    \"\"\"\n",
    "    This object represents the corpus.\n",
    "    Attributes:\n",
    "        docs_processed (int): Number of documents processed.\n",
    "        lexicon (dict): Dictionaryh object representing lexicon\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.docs_processed = 0\n",
    "        self.lexicon = {}\n",
    "        self.collection_freq = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Posting():\n",
    "    \"\"\"\n",
    "    This object represents one posting in a postings list.\n",
    "    Attributes:\n",
    "        doc_id (int): Document's id.\n",
    "        term_freq (int): Number of times the term occurs in the document.\n",
    "    \"\"\"\n",
    "    def __init__(self, doc_id, term_freq):\n",
    "        self.doc_id = doc_id\n",
    "        self.term_freq = term_freq\n",
    "    def __str__(self):\n",
    "        return f\"({self.doc_id},{self.term_freq})\"\n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Term():\n",
    "    \"\"\"\n",
    "    This object represents a term.\n",
    "    Attributes:\n",
    "        offset (int): The beginning offset for the postings list in a binary file.\n",
    "        id (str): The term id for this object\n",
    "        doc_freq (int): Number of documents containing this term.\n",
    "    \"\"\"\n",
    "    def __init__(self, term_id):\n",
    "        self.offset = 0\n",
    "        self.id = term_id\n",
    "        self.doc_freq = 0\n",
    "        self.idf = -1\n",
    "    def __str__(self):\n",
    "        return f\"doc_freq: {self.doc_freq}; offset: {self.offset}; id: {self.id}; idf: {self.idf}\"\n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File IO Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    \"\"\"\n",
    "    Reads file.\n",
    "    Parameters:\n",
    "        file_name (str): The filename for file to be read.\n",
    "    Returns:\n",
    "        content (str): contents of the file.\n",
    "    \"\"\"\n",
    "    with open(file_name) as file:\n",
    "        content = file.read()\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_lexicon_to_file(lexicon, filename):\n",
    "    \"\"\"\n",
    "    Write lexicon to a file.\n",
    "    Parameters:\n",
    "        lexicon (dict{str, Term}): Object representing the lexicon.\n",
    "        filename (str): Filename for the file created.\n",
    "    Returns:\n",
    "        N/A\n",
    "    \"\"\"\n",
    "    pickle.dump(lexicon, open(filename, \"wb\" ))\n",
    "    \n",
    "def read_lexicon_from_file(filename):\n",
    "    \"\"\"\n",
    "    Read lexicon from file.\n",
    "    Parameters:\n",
    "        filename (str): Filename for the file read.\n",
    "    Returns:\n",
    "        Lexicon object loaded from file\n",
    "    \"\"\"\n",
    "    return pickle.load(open(filename, \"rb\" )) \n",
    "\n",
    "if TEST_PICKLE_READ_WRITE:\n",
    "    test_obj = CorpusStats()\n",
    "    test_obj.num_paras = 7\n",
    "    write_corpus_to_file(test_obj, \"read_write_test.pk\")\n",
    "    read_test = read_corpus_from_file(\"read_write_test.pk\")\n",
    "    assert read_test.num_paras == 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_file_write(lexicon, postings, bin_file_name):\n",
    "    \"\"\"\n",
    "    Write postings list to two binary files and use offset to track the beginning of the binary file.\n",
    "    Attributes:\n",
    "        lexicon (dict{str: Term}): A dict representing the lexicon.\n",
    "        postings (dict{str: list[Posting]}): A list of Posting objects.\n",
    "        bin_file_name (str): Filename for binary file containing interleaving doc_ids and term_freqs as 32-bit ints.\n",
    "    Returns:\n",
    "        N/A\n",
    "    \"\"\"\n",
    "    bin_file = open(bin_file_name, \"wb\")\n",
    "    \n",
    "    assert bin_file.tell() == 0\n",
    "    offset = 0\n",
    "    for key, val in lexicon.items():\n",
    "        for posting in postings[val.id]:\n",
    "            bin_file.write((int(posting.doc_id)).to_bytes(NUM_BYTES, byteorder=BYTE_ORDER, signed=SIGNED))\n",
    "            bin_file.write((int(posting.term_freq)).to_bytes(NUM_BYTES, byteorder=BYTE_ORDER, signed=SIGNED))\n",
    "        val.offset = offset\n",
    "        offset = bin_file.tell()\n",
    "    \n",
    "    bin_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_top_100_scores(scores, filename):\n",
    "    file = open(filename, \"a\")\n",
    "    for qid, query_scores in scores.items():\n",
    "        try:\n",
    "            top_100 = dict(sorted(query_scores.items(), key=lambda k:k[1], reverse = True)[0:100])\n",
    "        except IndexError:\n",
    "            top_100 = dict(sorted(query_scores.items(), key=lambda k:k[1], reverse = True))\n",
    "        for rank, (doc_id, doc_score) in enumerate(top_100.items(), 1):\n",
    "            file.write(f\"{qid} Q0 {doc_id} {rank} {doc_score:.6f} skasim3\\n\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_punctuation(doc):\n",
    "    \"\"\"\n",
    "    Strips punctuation from a document.\n",
    "    Parameters:\n",
    "        doc (str): A string representing the document.\n",
    "    Returns:\n",
    "        stripped (str): A string that has been stripped of punctuation.\n",
    "        \n",
    "    \"\"\"    \n",
    "    separated = \"\"\n",
    "    for ch in doc:\n",
    "        if ch in PUNCTUATION and ch != \".\":\n",
    "            separated = f\"{separated} {ch} \"\n",
    "        else:\n",
    "            separated = f\"{separated}{ch}\"\n",
    "    return separated.strip()\n",
    "\n",
    "if RUN_TESTS:\n",
    "    assert separate_punctuation(\"That's my mother-in-law\") == \"That ' s my mother - in - law\"\n",
    "    assert separate_punctuation(\"\"\"NAC has developed a National HIV/AIDS/STI/TB Intervention Strategic Plan (2002-2005) that aims to reduce the HIV prevalence rate among Zambians from 19.3% to 11.7% and improve the health status of people living with HIV/AIDS by 2005.\"\"\") == \"\"\"NAC has developed a National HIV / AIDS / STI / TB Intervention Strategic Plan  ( 2002 - 2005 )  that aims to reduce the HIV prevalence rate among Zambians from 19.3 %  to 11.7 %  and improve the health status of people living with HIV / AIDS by 2005.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_doc(doc):\n",
    "    \"\"\"\n",
    "    Removes new lines, multiple periods (e.g., mercedes...awesome), punctuation, multiple spaces (e.g., mercedes    awesome), and lowers the case of the provided document.\n",
    "    Parameters:\n",
    "        doc (str): A string representing the document.\n",
    "    Returns:\n",
    "        multi_space_stripped_doc (str): A string stripped of new lines, multiple periods, punctuation, multiple spaces with case lowered.\n",
    "    \"\"\"\n",
    "    stripped_doc = doc.strip(\"\\n\").lower()\n",
    "    multi_period_stripped_doc = re.sub(\"\\.\\.+\", \" \", stripped_doc)\n",
    "    punctuation_separated_doc = separate_punctuation(multi_period_stripped_doc)\n",
    "    multi_space_stripped_doc = re.sub(\" +\", \" \", punctuation_separated_doc)\n",
    "    new_line_stripped_doc = re.sub(\"\\n\",\" \", multi_space_stripped_doc)\n",
    "    return new_line_stripped_doc.strip()\n",
    "\n",
    "if RUN_TESTS:\n",
    "    assert strip_doc(\"To Be? Or* #not To +% be T&HAT I^s...the 99$ question!\") == \"to be ? or * # not to + % be t & hat i ^ s the 99 $ question !\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_on_spaces(doc):\n",
    "    \"\"\"\n",
    "    Splits document based on space character into a list of words/terms and removes any digits so that they are not counted as a word, e.g. \"20\"\n",
    "    Parameters:\n",
    "        doc (str): A string.\n",
    "    Returns:\n",
    "        terms (list[str]): A list of strings, which are the individual word/term.\n",
    "    \"\"\"\n",
    "    terms = []\n",
    "    term = \"\"\n",
    "    for ch in doc:\n",
    "        if ch != ' ':\n",
    "            term = term + ch\n",
    "        else:\n",
    "            if term not in PUNCTUATION and term not in EN_STOP_WORDS: \n",
    "                terms.append(term)\n",
    "            term = \"\"\n",
    "    if term not in PUNCTUATION and term not in EN_STOP_WORDS:\n",
    "        terms.append(term)\n",
    "    return terms\n",
    "if RUN_TESTS:\n",
    "    assert split_on_spaces(\"to be or not to be that is the question\") == ['question']\n",
    "    assert split_on_spaces(\"99 bottles on the wall\") == ['99', 'bottles', 'wall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_on_ngrams(doc, ngram_len):\n",
    "    \"\"\"\n",
    "    Splits document based on space character into a list of words/terms and removes any digits so that they are not counted as a word, e.g. \"20\"\n",
    "    Parameters:\n",
    "        doc (str): A string.\n",
    "    Returns:\n",
    "        terms (list[str]): A list of strings, which are the individual word/term.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(doc)\n",
    "    ngrams_list = []\n",
    "    for token in tokens:\n",
    "        if token not in PUNCTUATION and token not in EN_STOP_WORDS and not token.isdigit():\n",
    "            if len(token) > ngram_len:\n",
    "                for t in [\"\".join(k) for k in list(ngrams(token,n=ngram_len))]:\n",
    "                    ngrams_list.append(t)\n",
    "            else:\n",
    "                ngrams_list.append(token)\n",
    "    return ngrams_list\n",
    "\n",
    "if RUN_TESTS:\n",
    "    assert split_on_ngrams(\"to be or not to be that is the question\", 5) == ['quest', 'uesti', 'estio', 'stion']\n",
    "    assert split_on_ngrams(\"99 bottles on the wall\", 5) == ['bottl', 'ottle', 'ttles', 'wall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_and_remove_doc_id(doc, is_doc=True):\n",
    "    \"\"\"\n",
    "    Utilize regex to capture document id and remove document id from the text, so that it is not processed.\n",
    "    Parameters:\n",
    "        doc (str): A string.\n",
    "    Returns:\n",
    "        doc_id (str): The document id.\n",
    "        modified_doc (str): Document with the pattern <p id = xx> removed.\n",
    "    \"\"\"\n",
    "    if is_doc:\n",
    "        pattern = r\"< p id = \\d+ >\"\n",
    "    else:\n",
    "        pattern = r\"< q id = \\d+ >\"\n",
    "    doc_id = re.findall(r\"\\d+\", doc)\n",
    "    modified_doc = re.sub(pattern, \"\", doc).strip(\"\\n\")\n",
    "    return doc_id[0], (re.sub(\" +\", \" \", modified_doc)).strip()\n",
    "\n",
    "if RUN_TESTS:\n",
    "    assert capture_and_remove_doc_id(\"99 bottles on the wall\")[1] == \"99 bottles on the wall\"\n",
    "    assert capture_and_remove_doc_id(\"< p id = 2 > 99 bottles on the wall\")[0][0] == \"2\"\n",
    "    assert capture_and_remove_doc_id(\"< p id = 27 > 99 bottles on the wall\")[1] == \"99 bottles on the wall\"\n",
    "    assert capture_and_remove_doc_id(\"< q id = 1 > 99 bottles on the wall\", is_doc=False)[1] == \"99 bottles on the wall\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicon Enrichment Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_lexicon(corpus, postings, words, doc_id):\n",
    "    \"\"\"\n",
    "    Enrich lexicon with list of words and document_id provided:\n",
    "        * If word is not in the lexicon, the word is added to lexicon, a Posting object is instantiated, and doc_freq is incremented by 1.\n",
    "        * If word is in lexicon, a check is done to see if the word reoccurs in the last indexed document:\n",
    "            - If the word reoccurs in the same document, the previously saved term_freq count for the document is incremented by 1.\n",
    "            - If the word does not reoccur in the same document, but is in the lexicon, then a new Posting object is created and doc_freq is incremented by 1.\n",
    "    Parameters:\n",
    "        lexicon (Corpus): An object representing the corpus.\n",
    "        postings (dict{str: list[Posting]}): A list of Posting objects.\n",
    "        words (list[str]): A list of words.\n",
    "        doc_id: The document id of the document from, which words was retrieved.\n",
    "    Returns:\n",
    "        lexicon (dict{str: Term}): Updated lexicon.\n",
    "        postings (dict{str: list[Posting]}): Updated postings.\n",
    "    \"\"\"\n",
    "\n",
    "    for word in words:\n",
    "        corpus.collection_freq += 1\n",
    "        if word not in corpus.lexicon:\n",
    "            posting = Posting(doc_id, 1)\n",
    "            increment_term_count()\n",
    "            term = Term(TERM_COUNT)\n",
    "            term.doc_freq = 1\n",
    "            postings[term.id] = [posting]\n",
    "            corpus.lexicon[word] = term\n",
    "        else:\n",
    "            prev_indexed_term_id = corpus.lexicon[word].id\n",
    "            last_doc_id = postings[prev_indexed_term_id][-1].doc_id\n",
    "            if last_doc_id == doc_id:\n",
    "                postings[prev_indexed_term_id][-1].term_freq += 1\n",
    "            else:\n",
    "                posting = Posting(doc_id, 1)\n",
    "                corpus.lexicon[word].doc_freq += 1\n",
    "                term_id = corpus.lexicon[word].id\n",
    "                postings[term_id].append(posting)\n",
    "\n",
    "    return corpus   \n",
    "\n",
    "if RUN_TESTS:\n",
    "    test_corpus = Corpus()\n",
    "    test_postings = {}\n",
    "    test_words = [\"you\", \"are\", \"you\"]\n",
    "    test_enriched = enrich_lexicon(test_corpus, test_postings, test_words, 77)\n",
    "    assert test_enriched.lexicon[\"you\"].doc_freq == 1\n",
    "    t1 = test_enriched.lexicon[\"you\"].id\n",
    "    assert str(test_postings[t1][0]) == \"(77,2)\"\n",
    "    assert test_enriched.lexicon[\"are\"].doc_freq == 1\n",
    "    t2 = test_enriched.lexicon[\"are\"].id\n",
    "    assert str(test_postings[t2][0]) == \"(77,1)\"\n",
    "    test_new_words = [\"you\", \"thirsty\"]\n",
    "    test_enriched_update = enrich_lexicon(test_corpus, test_postings, test_new_words, 32)\n",
    "    assert test_enriched_update.lexicon[\"you\"].doc_freq == 2\n",
    "    assert str(test_postings[t1][0]) == \"(77,2)\"\n",
    "    assert str(test_postings[t1][1]) == \"(32,1)\"\n",
    "    assert test_enriched_update.lexicon[\"thirsty\"].doc_freq == 1\n",
    "    t3 = test_enriched.lexicon[\"thirsty\"].id\n",
    "    assert str(test_postings[t3][0]) == \"(32,1)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_postings(postings):\n",
    "    \"\"\"\n",
    "    Sort postings list by doc_id.\n",
    "    Attributes:\n",
    "        postings(list[Posting]): A list of Posting objects.\n",
    "    Returns:\n",
    "        postings(list[Posting])\n",
    "    \"\"\"\n",
    "    for k, post in postings.items():\n",
    "        post.sort(key=lambda k:k.doc_id, reverse=False)\n",
    "    return postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_terms(lexicon):\n",
    "    \"\"\"\n",
    "    Sort terms in a lexicon in alphabetical order.\n",
    "    Attributes:\n",
    "        lexicon(dict{str: Term}). A dictionary representing the lexicon.\n",
    "    Returns:\n",
    "        sorted lexicon\n",
    "    \"\"\"\n",
    "    return dict(sorted(lexicon.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_postings_list_for_term(word, lexicon, binary_file):\n",
    "    \"\"\"\n",
    "    Retrieves posting list for a term.\n",
    "    Attributes:\n",
    "        word (str): The term being retrieved\n",
    "        lexicon (dict{str: Term}): A dict representing the lexicon.\n",
    "        doc_id_file (str): Filename for binary file containing doc_ids as 32-bit ints.\n",
    "        term_freq_file (str): Filename for binary file containing term_freqs as 32-bit ints.\n",
    "    Returns:\n",
    "        postings_list (list[Posting]): A list of Posting objects for a term.\n",
    "    \"\"\"\n",
    "    binary_file = open(binary_file, \"rb\")\n",
    "    \n",
    "    try:\n",
    "        postings_list = []\n",
    "        term = lexicon[word]\n",
    "        offset = term.offset\n",
    "        binary_file.seek(int(term.offset))\n",
    "        for _ in range(0, term.doc_freq):\n",
    "            bytes_read_doc_id = binary_file.read(NUM_BYTES)\n",
    "            bytes_read_term_freq = binary_file.read(NUM_BYTES)\n",
    "            doc_id = int.from_bytes(bytes_read_doc_id, byteorder=BYTE_ORDER,signed=SIGNED)\n",
    "            term_freq = int.from_bytes(bytes_read_term_freq, byteorder=BYTE_ORDER,signed=SIGNED)\n",
    "            posting = Posting(doc_id, term_freq)\n",
    "            postings_list.append(posting)\n",
    "        return postings_list\n",
    "    except KeyError as e:\n",
    "        return []\n",
    "    \n",
    "    binary_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Enrichment Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query_file(filename):\n",
    "    \"\"\"\n",
    "    Given the query filename, process and tokenize query terms in the exact same manner as the documents.\n",
    "    Attributes:\n",
    "        filename (str): Filename for the query file.\n",
    "    Returns:\n",
    "        queries (dict(int, list[str])): Return a dict for which the key is an int value representing query_id and \n",
    "        value is a list of strings representing the terms for that query.\n",
    "    \"\"\"\n",
    "    content = read_file(filename)\n",
    "    content = re.split(\"</Q>\", content)\n",
    "    queries_processed = 0\n",
    "    queries = {}\n",
    "    for query in content:\n",
    "        validation_check = re.findall(r\"\\d+\", query)\n",
    "        try:\n",
    "            validation_check[0]\n",
    "            clean_query = strip_doc(query)\n",
    "            query_id, modified_query = capture_and_remove_doc_id(clean_query, is_doc=False)\n",
    "            query_terms = split_on_ngrams(modified_query, NGRAM_LEN)\n",
    "            queries[query_id] = query_terms\n",
    "            queries_processed += 1\n",
    "        except IndexError:\n",
    "            pass\n",
    "    return queries\n",
    "\n",
    "if RUN_TESTS:\n",
    "    assert str(process_query_file(\"data/animal.topics.txt\")) == \"{'1': ['bird', 'cat', 'dog']}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_TESTS:\n",
    "    # For testing\n",
    "    test_binary_file = \"TEST_binary.bin\"\n",
    "    tc_lexicon = {}\n",
    "    red1 = Posting(3, 7)\n",
    "    red2 = Posting(5, 1)\n",
    "    red3 = Posting(7, 9)\n",
    "    redterm = Term(1)\n",
    "    redterm.offset = 16\n",
    "    redterm.doc_freq = 3\n",
    "    tc_lexicon[\"red\"] = redterm\n",
    "    blue1 = Posting(3, 5)\n",
    "    blue2 = Posting(7, 2)\n",
    "    blueterm = Term(2)\n",
    "    blueterm.offset = 0\n",
    "    blueterm.doc_freq = 2\n",
    "    tc_lexicon[\"blue\"] = blueterm\n",
    "    tc_corpus = Corpus()\n",
    "    tc_corpus.lexicon = tc_lexicon\n",
    "    tc_corpus.docs_processed = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_idf_for_term(corpus, term):\n",
    "    \"\"\"\n",
    "    Calculate IDF(t) = log_2(N/df(t)) for a term.\n",
    "    Attributes:\n",
    "        corpus (Corpus): The Corpus object.\n",
    "        term (str): String representing one term.\n",
    "    Returns:\n",
    "        float representing IDF for a term in lexicon.\n",
    "    \"\"\"\n",
    "    N = corpus.docs_processed\n",
    "    df_t = corpus.lexicon[term].doc_freq\n",
    "    return numpy.log2(N/df_t)\n",
    "\n",
    "if RUN_TESTS:\n",
    "    assert round(calc_idf_for_term(tc_corpus, \"red\"), 6) == .736966\n",
    "    assert round(calc_idf_for_term(tc_corpus, \"blue\"), 6) == 1.321928"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_idf_values_in_corpus(corpus):\n",
    "    \"\"\"\n",
    "    Given a corpus, loop through each term in the lexicon, calculate the term's IDF and update corpus.lexicon[term].idf\n",
    "    field.\n",
    "    Attributes:\n",
    "        corpus (Corpus): A corpus object.\n",
    "    Returns:\n",
    "        corpus (Corpus): The corpus.lexicon with the updated IDF values.\n",
    "    \"\"\"\n",
    "    for term in corpus.lexicon:\n",
    "        corpus.lexicon[term].idf = calc_idf_for_term(corpus, term)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_doc_vector_lengths(corpus, binary_file):\n",
    "    \"\"\"\n",
    "    Calculate vector lengths for a corpus. Since inverted index/postings list for a term is stored in a binary file,\n",
    "    read binary file and return posting list for a given term.\n",
    "    \"\"\"\n",
    "    doclens = defaultdict(float)\n",
    "    for term in corpus.lexicon:\n",
    "        postings_list = get_postings_list_for_term(term, corpus.lexicon, binary_file)\n",
    "        for posting in postings_list:\n",
    "            weight = int(posting.term_freq) * calc_idf_for_term(corpus, term)\n",
    "            doclens[int(posting.doc_id)] += weight * weight\n",
    "    for doc_id, summ in doclens.items():\n",
    "        doclens[doc_id] = numpy.sqrt(summ)\n",
    "    return dict(sorted(doclens.items()))\n",
    "\n",
    "\n",
    "if RUN_TESTS:\n",
    "    tc_doclens = calc_doc_vector_lengths(tc_corpus, test_binary_file)\n",
    "    assert round(tc_doclens[3], 4) == 8.3845\n",
    "    assert round(tc_doclens[5], 4) == .7370\n",
    "    assert round(tc_doclens[7], 4) == 7.1402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_query_tfs(corpus, query_terms):\n",
    "    \"\"\"\n",
    "    Calcuate term frequencies of a an array of query terms. If that term exists in the lexicon. If it doesn't exist in the lexicon\n",
    "    then ignore the term.\n",
    "    Attributes:\n",
    "        corpus (Corpus): A Corpus object\n",
    "        query_terms (list[str]): A list of string objects representing a query term.\n",
    "    Returns:\n",
    "        term_tfs (dict[str, int]): A dict object representing the term (str) and its term frequency (int).\n",
    "    \"\"\"\n",
    "    term_tfs = {}\n",
    "    for term in query_terms:\n",
    "        if term in corpus.lexicon:\n",
    "            if term in term_tfs:\n",
    "                term_tfs[term] += 1\n",
    "            else:\n",
    "                term_tfs[term] = 1\n",
    "    return term_tfs\n",
    "\n",
    "if RUN_TESTS:\n",
    "    qtc = \"want blue blue blue red\"\n",
    "    qtc = qtc.split()\n",
    "    qtc_tfs = calc_query_tfs(tc_corpus, qtc)\n",
    "    assert str(qtc_tfs) == \"{'blue': 3, 'red': 1}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_query_vector_length(corpus, query_tfs):\n",
    "    \"\"\"\n",
    "    Given the corpus and the query term frequencies, calculate the query vector length.\n",
    "    Attributes:\n",
    "        corpus (Corpus): A Corpus object.\n",
    "        query_tfs (dict): dict[str, int]): A dict object representing the term (str) and its term frequency (int).\n",
    "    Returns:\n",
    "        float representing the query vector length.\n",
    "    \"\"\"\n",
    "    sum_of_sqrs = 0\n",
    "    for term, tf_q in query_tfs.items():\n",
    "        weight = tf_q * calc_idf_for_term(corpus, term)\n",
    "        sum_of_sqrs += weight * weight\n",
    "    return numpy.sqrt(sum_of_sqrs)\n",
    "\n",
    "if RUN_TESTS:\n",
    "    assert round(calc_query_vector_length(tc_corpus, qtc_tfs), 4) == 4.0337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_query_tfs_veclens_by_qid(corpus, queries_dict):\n",
    "    \"\"\"\n",
    "    Given the corpus and the queries_dict that has query_id as key and list of query terms as value, utilize the IDF\n",
    "    value for a term in the corpus to calculate the query term frequences.\n",
    "    Attributes:\n",
    "        corpus (Corpus): A Corpus object.\n",
    "        queries (dict(int, list[str])): Return a dict for which the key is an int value representing query_id and \n",
    "        value is a list of strings representing the terms for that query.\n",
    "    Returns:\n",
    "        q_tfs (dict[int, list[int, flot]]): A dict with query_id as key and value is a list of len 2, with query_tf\n",
    "        at index 0 and query_vectorlength at index 1.\n",
    "    \"\"\"\n",
    "    q_tfs_veclens = {}\n",
    "    for qid, qterms in queries_dict.items():\n",
    "        q_tf = calc_query_tfs(corpus, qterms)\n",
    "        q_veclen = calc_query_vector_length(corpus, q_tf)\n",
    "        q_tfs_veclens[qid] = [q_tf, q_veclen] # {query id: [query term frequency, query veclen]}\n",
    "    return q_tfs_veclens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_documents_for_cos_sim(corpus, query_tfs_veclens_by_qid, doclens, binary_file):\n",
    "    scores_by_qid = {}\n",
    "    for qid, qterm in query_tfs_veclens_by_qid.items():\n",
    "        score = defaultdict(float)\n",
    "        for term in qterm[0]:\n",
    "            try:\n",
    "                postings_list = get_postings_list_for_term(term, corpus.lexicon, binary_file)\n",
    "                for posting in postings_list:\n",
    "                    score[posting.doc_id] += query_tfs_veclens_by_qid[qid][0][term] * corpus.lexicon[term].idf * posting.term_freq * corpus.lexicon[term].idf\n",
    "            except KeyError:\n",
    "                pass\n",
    "        for doc_id, value in score.items():\n",
    "            score[doc_id] /= (doclens[float(doc_id)] * query_tfs_veclens_by_qid[qid][1]) if (doclens[float(doc_id)] * query_tfs_veclens_by_qid[qid][1]) !=0 else .00000000000000001 # account for div by 0\n",
    "        scores_by_qid[qid] = score\n",
    "    return scores_by_qid\n",
    "\n",
    "if RUN_TESTS:\n",
    "    # for correctness of cosine scores, check assertions below\n",
    "    tc_corpus = populate_idf_values_in_corpus(tc_corpus)\n",
    "    assert round(tc_corpus.lexicon[\"red\"].idf, 6) == .736966\n",
    "    assert round(tc_corpus.lexicon[\"blue\"].idf, 6) == 1.321928\n",
    "    tc_doclens = calc_doc_vector_lengths(tc_corpus, test_binary_file)\n",
    "    assert round(tc_doclens[3], 4) == 8.3845\n",
    "    assert round(tc_doclens[5], 4) == .7370\n",
    "    assert round(tc_doclens[7], 4) == 7.1402\n",
    "    qtc2 = \"want blue blue blue red\"\n",
    "    qtc2 = qtc2.split()\n",
    "    qtc_dict2 = {\"1\":qtc2}\n",
    "    tc_query_veclens_by_qid = calculate_query_tfs_veclens_by_qid(tc_corpus, qtc_dict2)\n",
    "    assert str(tc_query_veclens_by_qid[\"1\"][0]) == \"{'blue': 3, 'red': 1}\"\n",
    "    assert round(tc_query_veclens_by_qid[\"1\"][1], 4) == 4.0337\n",
    "    tc_scores = score_documents_for_cos_sim(tc_corpus, tc_query_veclens_by_qid, tc_doclens, test_binary_file)\n",
    "    assert round(tc_scores[\"1\"][3], 4) == .8875\n",
    "    assert round(tc_scores[\"1\"][5], 4) == .1827\n",
    "    assert round(tc_scores[\"1\"][7], 4) == .5338"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Lexicon and Postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index(filename, binary_file_name, lexicon_file_name):\n",
    "    \"\"\"\n",
    "    Create binary inverted index for file read. Split file based on \"</P>\" into individual documents. \n",
    "    Conduct a validation check to ensure document can be parsed. Enrich document. Create two binary files\n",
    "    Parameters:\n",
    "        filename: Filename representing the file consumed.\n",
    "        binary_file_name: Name of binary file to which interleaving  32-bit int doc_ids and term_freqs are written to.\n",
    "        lexicon_file_name: Name of .pk file to which the lexicon object is written to.\n",
    "    Returns:\n",
    "        corpus (Corpus): Object represents corpus generated from the file consumed.\n",
    "    \"\"\"\n",
    "    corpus = Corpus()\n",
    "    postings = {}\n",
    "    content = read_file(filename)\n",
    "    content = re.split(\"</P>\", content)\n",
    "    docs_processed = 0\n",
    "    for doc in content:\n",
    "        validation_check = re.findall(r\"\\d+\", doc)\n",
    "        try: \n",
    "            validation_check[0]\n",
    "            clean_doc = strip_doc(doc)\n",
    "            doc_id, modified_doc = capture_and_remove_doc_id(clean_doc)\n",
    "            terms = split_on_ngrams(modified_doc, NGRAM_LEN)\n",
    "            enrich_lexicon(corpus, postings, terms, doc_id) \n",
    "            docs_processed += 1\n",
    "        except IndexError:\n",
    "            pass\n",
    "    sort_postings(postings)\n",
    "    sorted_lexicon = sort_terms(corpus.lexicon)\n",
    "    corpus.lexicon = sorted_lexicon\n",
    "    corpus.docs_processed = docs_processed\n",
    "    # write postings dict to .bin file\n",
    "    binary_file_write(corpus.lexicon, postings, binary_file_name)\n",
    "    print(f\"Total docs processed: {corpus.docs_processed}\")\n",
    "    populate_idf_values_in_corpus(corpus)\n",
    "    # write corpus to .pk file\n",
    "    write_lexicon_to_file(corpus, lexicon_file_name)\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Inverted Index - Write Binary File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = \"5gram/5gram_inverted_index.bin\"\n",
    "lexicon_file = \"5gram/5gram_corpus.pk\"\n",
    "ranked_a = \"5gram/5gram_rankedlist_a.txt\"\n",
    "ranked_b = \"5gram/5gram_rankedlist_b.txt\"\n",
    "\n",
    "text_file = \"data/animal.txt\"\n",
    "cord19_topics_keyword = \"data/animal.topics.txt\"\n",
    "cord19_topics_questions = \"data/animal.topics.txt\"\n",
    "\n",
    "# text_file = \"data/cord19.txt\"\n",
    "# cord19_topics_keyword = \"data/cord19.topics.keyword.txt\"\n",
    "# cord19_topics_questions = \"data/cord19.topics.question.txt\"\n",
    "\n",
    "start = time.time()\n",
    "# Write .bin and .pk files\n",
    "corpus = create_inverted_index(text_file, inverted_index, lexicon_file)\n",
    "end = time.time()\n",
    "\n",
    "elapsed = end - start\n",
    "mins = elapsed // 60\n",
    "secs = elapsed - (60*mins)\n",
    "print(f\"Run time to create inverted index: {round(mins)}m {round(secs, 2)}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_by_qid(corpus, binary_file, query_file):\n",
    "    doclens = calc_doc_vector_lengths(corpus, inverted_index)\n",
    "    queries = process_query_file(query_file)\n",
    "    \n",
    "    tf_idfs1 = calc_query_tfs(corpus, queries[\"1\"])\n",
    "    print(\"TF/IDF weights for each query term in first query\")\n",
    "    for t, qtf in tf_idfs1.items():\n",
    "        print(f\"  {t}: {qtf * calc_idf_for_term(corpus, t)}\")\n",
    "    \n",
    "    query_tfs_veclens_by_qid = calculate_query_tfs_veclens_by_qid(corpus, queries)\n",
    "    \n",
    "    start = time.time()\n",
    "    scores_by_qid = score_documents_for_cos_sim(corpus, query_tfs_veclens_by_qid, doclens, binary_file)\n",
    "    end = time.time()\n",
    "    \n",
    "    elapsed = end - start\n",
    "    mins = elapsed // 60\n",
    "    secs = elapsed - (60*mins)\n",
    "    print(f\"All queries scored in: {round(mins)}m {round(secs, 2)}s\\t\")\n",
    "    return scores_by_qid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Ranked List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing keyword topic queries...\")\n",
    "scores_by_qid = get_scores_by_qid(corpus, inverted_index, cord19_topics_keyword)\n",
    "rank_top_100_scores(scores_by_qid, ranked_a)\n",
    "print(\"Keyword topic queries processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nProcessing question topic queries...\")\n",
    "scores_by_qid = get_scores_by_qid(corpus, inverted_index, cord19_topics_questions)\n",
    "rank_top_100_scores(scores_by_qid, ranked_b)\n",
    "print(\"Question topic queries processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = read_lexicon_from_file(lexicon_file)\n",
    "lexicon = corpus.lexicon\n",
    "print(f\"\\nNumber of docs indexed: {corpus.docs_processed}\")\n",
    "collection_size = corpus.collection_freq\n",
    "vocab_size = len(lexicon)\n",
    "print(f\"Collection size: {collection_size}\")\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "bin_file_size = os.path.getsize(inverted_index)\n",
    "lex_size = os.path.getsize(lexicon_file)\n",
    "print(f\"inverted_index.bin size: {round(bin_file_size/(1024*1024.0), 2)} MB\")\n",
    "print(f\"lexicon.pk size: {round(lex_size/(1024*1024.0), 2)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(read_lexicon_from_file(lexicon_file).lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIR Query Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4gram\n"
     ]
    }
   ],
   "source": [
    "gram = f\"{NGRAM_LEN}gram\"\n",
    "print(gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = f\"{gram}/{gram}_inverted_index.bin\"\n",
    "lexicon_file = f\"{gram}/{gram}_corpus.pk\"\n",
    "corpus = read_lexicon_from_file(lexicon_file)\n",
    "lexicon = corpus.lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out keywords\n",
    "ranked_en_base_keywords = f\"{gram}/ranked/{gram}_rankedlist_en_base_keywords.txt\"\n",
    "ranked_hi2en_gt_keywords = f\"{gram}/ranked/{gram}_rankedlist_hi2en_gt_keywords.txt\"\n",
    "# ranked_hi2en_gt_synset_keywords = f\"{gram}/ranked/{gram}_rankedlist_hi2en_gt_synset_keywords.txt\"\n",
    "ranked_hi2en_emb_keywords = f\"{gram}/ranked/{gram}_rankedlist_hi2en_emb_keywords.txt\"\n",
    "ranked_hi2en_emb_synset_keywords = f\"{gram}/ranked/{gram}_rankedlist_hi2en_emb_synset_keywords.txt\"\n",
    "ranked_hi2en_emb_no_oov_keywords = f\"{gram}/ranked/{gram}_rankedlist_hi2en_emb_no_oov_keywords.txt\"\n",
    "ranked_hi2en_gt_emb_synset_translit_keywords = f\"{gram}/ranked/{gram}_rankedlist_hi2en_gt_emb_synset_translit_keywords.txt\"\n",
    "\n",
    "# out questions\n",
    "ranked_en_base_questions = f\"{gram}/ranked/{gram}_rankedlist_en_base_questions.txt\"\n",
    "ranked_hi2en_gt_questions = f\"{gram}/ranked/{gram}_rankedlist_hi2en_gt_questions.txt\"\n",
    "# ranked_hi2en_gt_synset_questions = f\"{gram}/ranked/{gram}_rankedlist_hi2en_gt_synset_questions.txt\"\n",
    "ranked_hi2en_emb_questions = f\"{gram}/ranked/{gram}_rankedlist_hi2en_emb_questions.txt\"\n",
    "ranked_hi2en_emb_synset_questions = f\"{gram}/ranked/{gram}_rankedlist_hi2en_emb_synset_questions.txt\"\n",
    "ranked_hi2en_emb_no_oov_questions = f\"{gram}/ranked/{gram}_rankedlist_hi2en_emb_no_oov_questions.txt\"\n",
    "ranked_hi2en_gt_emb_synset_translit_questions = f\"{gram}/ranked/{gram}_rankedlist_hi2en_gt_emb_synset_translit_questions.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_files = [\n",
    "    [files.cord19_topics_keyword, ranked_en_base_keywords],\n",
    "    [files.cord19_topics_keyword_HIN2ENG_GT, ranked_hi2en_gt_keywords],\n",
    "    [files.cord19_topics_keyword_HIN2ENG_EMB, ranked_hi2en_emb_keywords],\n",
    "    [files.cord19_topics_keyword_HIN2ENG_EMB_SYNSET, ranked_hi2en_emb_synset_keywords],\n",
    "    [files.cord19_topics_keyword_HIN2ENG_EMB_NO_OOV, ranked_hi2en_emb_no_oov_keywords],\n",
    "    [files.cord19_topics_keyword_HIN2ENG_GT_EMB_SYNSET_TRANSLIT, ranked_hi2en_gt_emb_synset_translit_keywords]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_files = [\n",
    "    [files.cord19_topics_questions, ranked_en_base_questions],\n",
    "    [files.cord19_topics_questions_HIN2ENG_GT, ranked_hi2en_gt_questions],\n",
    "    [files.cord19_topics_questions_HIN2ENG_EMB, ranked_hi2en_emb_questions],\n",
    "    [files.cord19_topics_questions_HIN2ENG_EMB_SYNSET, ranked_hi2en_emb_synset_questions],\n",
    "    [files.cord19_topics_questions_HIN2ENG_EMB_NO_OOV, ranked_hi2en_emb_no_oov_questions],\n",
    "    [files.cord19_topics_questions_HIN2ENG_GT_EMB_SYNSET_TRANSLIT, ranked_hi2en_gt_emb_synset_translit_questions]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hi2en ranked list keywords for terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gram: 4gram\n",
      "Processing keyword topic queries ['data/cord19.topics.keyword.txt', '4gram/ranked/4gram_rankedlist_en_base_keywords.txt']...\n",
      "TF/IDF weights for each query term in first query\n",
      "  coro: 1.8462095025478793\n",
      "  oron: 1.8377685518520566\n",
      "  rona: 1.8287599980154576\n",
      "  onav: 1.917000464045291\n",
      "  navi: 1.8763141496291873\n",
      "  avir: 1.81544473004592\n",
      "  viru: 1.2749693156788926\n",
      "  irus: 1.2855451370817648\n",
      "  orig: 4.095127733532502\n",
      "  rigi: 4.069560964126937\n",
      "  igin: 4.116435936685577\n",
      "All queries scored in: 2m 53.03s\t\n",
      "Keyword topic queries processing complete in 9.0m 54.933043003082275s.\n",
      "\n",
      "Processing keyword topic queries ['data/hi/parsed/cord19.topics.keyword.hi2eng.gt.txt', '4gram/ranked/4gram_rankedlist_hi2en_gt_keywords.txt']...\n",
      "TF/IDF weights for each query term in first query\n",
      "  koro: 11.50014022915963\n",
      "  oron: 1.8377685518520566\n",
      "  rona: 1.8287599980154576\n",
      "  onav: 1.917000464045291\n",
      "  navi: 1.8763141496291873\n",
      "  avir: 1.81544473004592\n",
      "  viru: 1.2749693156788926\n",
      "  irus: 1.2855451370817648\n",
      "  orig: 4.095127733532502\n",
      "  rigi: 4.069560964126937\n",
      "  igin: 4.116435936685577\n",
      "All queries scored in: 2m 12.92s\t\n",
      "Keyword topic queries processing complete in 8.0m 20.545710802078247s.\n",
      "\n",
      "Processing keyword topic queries ['data/hi/parsed/cord19.topics.keyword.hi2eng.emb.txt', '4gram/ranked/4gram_rankedlist_hi2en_emb_keywords.txt']...\n",
      "TF/IDF weights for each query term in first query\n",
      "  coxs: 8.61675638643574\n",
      "  oxsa: 8.628654969682309\n",
      "  xsac: 8.637643752909565\n",
      "  sack: 8.45177720759823\n",
      "  acki: 5.26899179243136\n",
      "  ckie: 8.530513878203148\n",
      "  kiev: 9.227121734753213\n",
      "  ievi: 6.604955134203389\n",
      "  evir: 8.516628351948198\n",
      "  viru: 3.824907947036678\n",
      "  irus: 3.8566354112452945\n",
      "  pole: 9.494685799067522\n",
      "  oler: 6.003437733168559\n",
      "  lero: 6.601286952616529\n",
      "  erov: 6.872993281806198\n",
      "  rovi: 2.011752301776899\n",
      "  ovir: 7.625582115620482\n",
      "  erbo: 9.11408179685255\n",
      "  rbov: 9.076928798435086\n",
      "  bovi: 5.965218410938069\n",
      "  orig: 12.285383200597506\n",
      "  rigi: 12.20868289238081\n",
      "  igin: 8.232871873371154\n",
      "  gina: 4.49315544883056\n",
      "  inal: 2.667250214994888\n",
      "  igio: 8.481139267229574\n",
      "  gion: 3.3811205290256385\n",
      "  iona: 1.5072018929938154\n",
      "  onal: 1.4295313519679538\n",
      "All queries scored in: 2m 37.14s\t\n",
      "Keyword topic queries processing complete in 8.0m 44.03803896903992s.\n",
      "\n",
      "Processing keyword topic queries ['data/hi/parsed/cord19.topics.keyword.hi2eng.emb_synset.txt', '4gram/ranked/4gram_rankedlist_hi2en_emb_synset_keywords.txt']...\n",
      "TF/IDF weights for each query term in first query\n",
      "  coxs: 17.23351277287148\n",
      "  oxsa: 17.257309939364617\n",
      "  xsac: 17.27528750581913\n",
      "  sack: 16.90355441519646\n",
      "  acki: 10.53798358486272\n",
      "  ckie: 17.061027756406297\n",
      "  kiev: 18.454243469506427\n",
      "  ievi: 13.209910268406778\n",
      "  evir: 17.033256703896395\n",
      "  viru: 10.19975452543114\n",
      "  irus: 10.284361096654118\n",
      "  pole: 9.494685799067522\n",
      "  oler: 6.003437733168559\n",
      "  lero: 6.601286952616529\n",
      "  erov: 6.872993281806198\n",
      "  rovi: 2.011752301776899\n",
      "  ovir: 11.438373173430723\n",
      "  erbo: 9.11408179685255\n",
      "  rbov: 9.076928798435086\n",
      "  bovi: 5.965218410938069\n",
      "  orig: 12.285383200597506\n",
      "  rigi: 12.20868289238081\n",
      "  igin: 8.232871873371154\n",
      "  gina: 4.49315544883056\n",
      "  inal: 2.667250214994888\n",
      "  igio: 8.481139267229574\n",
      "  gion: 3.3811205290256385\n",
      "  iona: 1.5072018929938154\n",
      "  onal: 1.4295313519679538\n",
      "  hepa: 4.50048181671678\n",
      "  epat: 4.621392597896815\n",
      "  pati: 1.3545168050288594\n",
      "  atit: 4.795293409495756\n",
      "  titi: 3.7340631275609253\n",
      "  itis: 3.220072492540765\n",
      "  sino: 7.543126154125274\n",
      "  inov: 6.156517063172948\n",
      "  novi: 5.221198059237912\n",
      "  quar: 4.703166566635742\n",
      "  uara: 5.212777636262612\n",
      "  aran: 4.226132764897493\n",
      "  ranj: 11.900678158743357\n",
      "  anja: 11.354709789638065\n",
      "  njav: 15.22260625363072\n",
      "  javi: 12.900678158743357\n",
      "  avir: 1.81544473004592\n",
      "  coro: 1.8462095025478793\n",
      "  oron: 1.8377685518520566\n",
      "  rona: 1.8287599980154576\n",
      "  supe: 9.677803922313336\n",
      "  uper: 9.652431076077644\n",
      "  perb: 19.487268897195555\n",
      "  erbu: 19.423288668706682\n",
      "  rbub: 35.08906869703617\n",
      "  bubb: 20.48150720068196\n",
      "  ubbl: 20.121437142507652\n",
      "  bble: 19.089068697036165\n",
      "  bles: 4.698652314134022\n",
      "  excl: 5.163802735835088\n",
      "  xclu: 5.164614530871544\n",
      "  clus: 1.9862930265879153\n",
      "  lusi: 2.078808531653308\n",
      "  usiv: 5.698652314134022\n",
      "  sivi: 7.010231466063452\n",
      "  ivit: 2.6804933320430737\n",
      "  vity: 2.8653291789879116\n",
      "  cont: 1.2764310129071617\n",
      "  onte: 3.752862698438277\n",
      "  nten: 3.197951363220386\n",
      "  tent: 1.806310115367227\n",
      "  ent—: 11.737179426460479\n",
      "  nt—i: 14.544534348518082\n",
      "  t—in: 13.737179426460479\n",
      "  tran: 1.7878216730099845\n",
      "  rans: 1.8540452282894755\n",
      "  ansf: 4.160695079663627\n",
      "  nsfe: 4.847349565293567\n",
      "  sfer: 5.092550571287892\n",
      "  fere: 1.8704524498817587\n",
      "  eren: 1.872855078064234\n",
      "  renc: 2.6126426516936787\n",
      "  ence: 1.0615531225250814\n",
      "  nces: 2.3915365752227715\n",
      "  llon: 9.816613893954884\n",
      "  thag: 13.959571847796926\n",
      "  hago: 6.930744884045502\n",
      "  agor: 12.335080982889133\n",
      "  ownd: 15.959571847796926\n",
      "  immi: 23.158726823968678\n",
      "  mmig: 26.616327019639762\n",
      "  migr: 18.210964233972415\n",
      "  igra: 17.70880239805118\n",
      "  gran: 16.028150195550484\n",
      "  rant: 12.203364433735848\n",
      "  ante: 4.464882904373299\n",
      "  nted: 2.6678255669629873\n",
      "  ants: 3.232644154667468\n",
      "All queries scored in: 6m 29.34s\t\n",
      "Keyword topic queries processing complete in 12.0m 53.0397367477417s.\n",
      "\n",
      "Processing keyword topic queries ['data/hi/parsed/cord19.topics.keyword.hi2eng.emb_no_oov.txt', '4gram/ranked/4gram_rankedlist_hi2en_emb_no_oov_keywords.txt']...\n",
      "TF/IDF weights for each query term in first query\n",
      "  coxs: 8.61675638643574\n",
      "  oxsa: 8.628654969682309\n",
      "  xsac: 8.637643752909565\n",
      "  sack: 8.45177720759823\n",
      "  acki: 5.26899179243136\n",
      "  ckie: 8.530513878203148\n",
      "  kiev: 9.227121734753213\n",
      "  ievi: 6.604955134203389\n",
      "  evir: 8.516628351948198\n",
      "  viru: 3.824907947036678\n",
      "  irus: 3.8566354112452945\n",
      "  pole: 9.494685799067522\n",
      "  oler: 6.003437733168559\n",
      "  lero: 6.601286952616529\n",
      "  erov: 6.872993281806198\n",
      "  rovi: 2.011752301776899\n",
      "  ovir: 7.625582115620482\n",
      "  erbo: 9.11408179685255\n",
      "  rbov: 9.076928798435086\n",
      "  bovi: 5.965218410938069\n",
      "  orig: 12.285383200597506\n",
      "  rigi: 12.20868289238081\n",
      "  igin: 8.232871873371154\n",
      "  gina: 4.49315544883056\n",
      "  inal: 2.667250214994888\n",
      "  igio: 8.481139267229574\n",
      "  gion: 3.3811205290256385\n",
      "  iona: 1.5072018929938154\n",
      "  onal: 1.4295313519679538\n",
      "All queries scored in: 2m 33.02s\t\n",
      "Keyword topic queries processing complete in 8.0m 53.793429136276245s.\n",
      "\n",
      "Processing keyword topic queries ['data/hi/parsed/cord19.topics.keyword.hi2eng.gt.emb.synset.translit.txt', '4gram/ranked/4gram_rankedlist_hi2en_gt_emb_synset_translit_keywords.txt']...\n",
      "TF/IDF weights for each query term in first query\n",
      "  koro: 11.50014022915963\n",
      "  oron: 3.675537103704113\n",
      "  rona: 3.6575199960309153\n",
      "  onav: 1.917000464045291\n",
      "  navi: 1.8763141496291873\n",
      "  avir: 3.63088946009184\n",
      "  viru: 7.649815894073356\n",
      "  irus: 7.713270822490589\n",
      "  orig: 12.285383200597506\n",
      "  rigi: 12.20868289238081\n",
      "  igin: 8.232871873371154\n",
      "  coxs: 8.61675638643574\n",
      "  oxsa: 8.628654969682309\n",
      "  xsac: 8.637643752909565\n",
      "  sack: 8.45177720759823\n",
      "  acki: 5.26899179243136\n",
      "  ckie: 8.530513878203148\n",
      "  kiev: 9.227121734753213\n",
      "  ievi: 6.604955134203389\n",
      "  evir: 8.516628351948198\n",
      "  pole: 9.494685799067522\n",
      "  oler: 6.003437733168559\n",
      "  lero: 6.601286952616529\n",
      "  erov: 6.872993281806198\n",
      "  rovi: 2.011752301776899\n",
      "  ovir: 11.438373173430723\n",
      "  erbo: 9.11408179685255\n",
      "  rbov: 9.076928798435086\n",
      "  bovi: 5.965218410938069\n",
      "  gina: 4.49315544883056\n",
      "  inal: 2.667250214994888\n",
      "  igio: 8.481139267229574\n",
      "  gion: 3.3811205290256385\n",
      "  iona: 1.5072018929938154\n",
      "  onal: 1.4295313519679538\n",
      "  hepa: 4.50048181671678\n",
      "  epat: 4.621392597896815\n",
      "  pati: 1.3545168050288594\n",
      "  atit: 4.795293409495756\n",
      "  titi: 3.7340631275609253\n",
      "  itis: 3.220072492540765\n",
      "  sino: 7.543126154125274\n",
      "  inov: 6.156517063172948\n",
      "  novi: 5.221198059237912\n",
      "  quar: 4.703166566635742\n",
      "  uara: 5.212777636262612\n",
      "  aran: 4.226132764897493\n",
      "  ranj: 11.900678158743357\n",
      "  anja: 11.354709789638065\n",
      "  njav: 15.22260625363072\n",
      "  javi: 12.900678158743357\n",
      "  coro: 1.8462095025478793\n",
      "  supe: 9.677803922313336\n",
      "  uper: 9.652431076077644\n",
      "  perb: 19.487268897195555\n",
      "  erbu: 19.423288668706682\n",
      "  rbub: 35.08906869703617\n",
      "  bubb: 20.48150720068196\n",
      "  ubbl: 20.121437142507652\n",
      "  bble: 19.089068697036165\n",
      "  bles: 4.698652314134022\n",
      "  excl: 5.163802735835088\n",
      "  xclu: 5.164614530871544\n",
      "  clus: 1.9862930265879153\n",
      "  lusi: 2.078808531653308\n",
      "  usiv: 5.698652314134022\n",
      "  sivi: 7.010231466063452\n",
      "  ivit: 2.6804933320430737\n",
      "  vity: 2.8653291789879116\n",
      "  cont: 1.2764310129071617\n",
      "  onte: 3.752862698438277\n",
      "  nten: 3.197951363220386\n",
      "  tent: 1.806310115367227\n",
      "  ent—: 11.737179426460479\n",
      "  nt—i: 14.544534348518082\n",
      "  t—in: 13.737179426460479\n",
      "  tran: 1.7878216730099845\n",
      "  rans: 1.8540452282894755\n",
      "  ansf: 4.160695079663627\n",
      "  nsfe: 4.847349565293567\n",
      "  sfer: 5.092550571287892\n",
      "  fere: 1.8704524498817587\n",
      "  eren: 1.872855078064234\n",
      "  renc: 2.6126426516936787\n",
      "  ence: 1.0615531225250814\n",
      "  nces: 2.3915365752227715\n",
      "  llon: 9.816613893954884\n",
      "  thag: 13.959571847796926\n",
      "  hago: 6.930744884045502\n",
      "  agor: 12.335080982889133\n",
      "  ownd: 15.959571847796926\n",
      "  immi: 23.158726823968678\n",
      "  mmig: 26.616327019639762\n",
      "  migr: 18.210964233972415\n",
      "  igra: 17.70880239805118\n",
      "  gran: 16.028150195550484\n",
      "  rant: 12.203364433735848\n",
      "  ante: 4.464882904373299\n",
      "  nted: 2.6678255669629873\n",
      "  ants: 3.232644154667468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All queries scored in: 6m 28.75s\t\n",
      "Keyword topic queries processing complete in 12.0m 36.53210473060608s.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"gram: {gram}\")\n",
    "for kwfile in keyword_files:\n",
    "    print(f\"Processing keyword topic queries {kwfile}...\")\n",
    "    start = time.time()\n",
    "    scores_by_qid = get_scores_by_qid(corpus, inverted_index, kwfile[0])\n",
    "    rank_top_100_scores(scores_by_qid, kwfile[1])\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    mins = elapsed // 60\n",
    "    secs = elapsed - (60*mins)\n",
    "    print(f\"Keyword topic queries processing complete in {mins}m {secs}s.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hi2en ranked list questions for terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing question topic queries ['data/cord19.topics.question.txt', '4gram/ranked/4gram_rankedlist_en_base_questions.txt']...\n",
      "TF/IDF weights for each query term in first query\n",
      "  orig: 4.095127733532502\n",
      "  rigi: 4.069560964126937\n",
      "  igin: 4.116435936685577\n",
      "  covi: 1.3366049030064162\n",
      "  ovid: 0.9389535536710554\n",
      "All queries scored in: 4m 47.27s\t\n",
      "Question topic queries processing complete in 11.0m 18.378764867782593s.\n",
      "\n",
      "Processing question topic queries ['data/hi/parsed/cord19.topics.question.hi2eng.gt.txt', '4gram/ranked/4gram_rankedlist_hi2en_gt_questions.txt']...\n",
      "TF/IDF weights for each query term in first query\n",
      "  orig: 4.095127733532502\n",
      "  rigi: 4.069560964126937\n",
      "  igin: 4.116435936685577\n",
      "  id19: 7.302551198823754\n",
      "All queries scored in: 4m 3.9s\t\n",
      "Question topic queries processing complete in 10.0m 33.96023488044739s.\n",
      "\n",
      "Processing question topic queries ['data/hi/parsed/cord19.topics.question.hi2eng.emb.txt', '4gram/ranked/4gram_rankedlist_hi2en_emb_questions.txt']...\n",
      "TF/IDF weights for each query term in first query\n",
      "  kovi: 10.364625258503148\n",
      "  ovid: 0.9389535536710554\n",
      "  orig: 12.285383200597506\n",
      "  rigi: 12.20868289238081\n",
      "  igin: 12.349307810056732\n",
      "  gins: 6.509047897225928\n",
      "  gina: 4.49315544883056\n",
      "  inat: 2.3479706411004333\n",
      "  nate: 2.99541150869659\n",
      "  ated: 0.8903178991737719\n",
      "  wond: 9.737179426460479\n",
      "  onde: 5.253939460435512\n",
      "  nder: 2.017607254810786\n",
      "  deri: 3.729851754559513\n",
      "  erin: 2.9552928507348284\n",
      "  ring: 1.9937575073012348\n",
      "All queries scored in: 10m 27.67s\t\n",
      "Question topic queries processing complete in 17.0m 1.0519521236419678s.\n",
      "\n",
      "Processing question topic queries ['data/hi/parsed/cord19.topics.question.hi2eng.emb_synset.txt', '4gram/ranked/4gram_rankedlist_hi2en_emb_synset_questions.txt']...\n",
      "TF/IDF weights for each query term in first query\n",
      "  kovi: 10.364625258503148\n",
      "  ovid: 0.9389535536710554\n",
      "  orig: 12.285383200597506\n",
      "  rigi: 12.20868289238081\n",
      "  igin: 12.349307810056732\n",
      "  gins: 6.509047897225928\n",
      "  gina: 4.49315544883056\n",
      "  inat: 2.3479706411004333\n",
      "  nate: 2.99541150869659\n",
      "  ated: 0.8903178991737719\n",
      "  wond: 9.737179426460479\n",
      "  onde: 5.253939460435512\n",
      "  nder: 2.017607254810786\n",
      "  deri: 3.729851754559513\n",
      "  erin: 2.9552928507348284\n",
      "  ring: 1.9937575073012348\n",
      "All queries scored in: 9m 35.17s\t\n",
      "Question topic queries processing complete in 16.0m 39.82656002044678s.\n",
      "\n",
      "Processing question topic queries ['data/hi/parsed/cord19.topics.question.hi2eng.emb_no_oov.txt', '4gram/ranked/4gram_rankedlist_hi2en_emb_no_oov_questions.txt']...\n",
      "TF/IDF weights for each query term in first query\n",
      "  orig: 12.285383200597506\n",
      "  rigi: 12.20868289238081\n",
      "  igin: 12.349307810056732\n",
      "  gins: 6.509047897225928\n",
      "  gina: 4.49315544883056\n",
      "  inat: 2.3479706411004333\n",
      "  nate: 2.99541150869659\n",
      "  ated: 0.8903178991737719\n",
      "  wond: 9.737179426460479\n",
      "  onde: 5.253939460435512\n",
      "  nder: 2.017607254810786\n",
      "  deri: 3.729851754559513\n",
      "  erin: 2.9552928507348284\n",
      "  ring: 1.9937575073012348\n",
      "All queries scored in: 8m 49.28s\t\n",
      "Question topic queries processing complete in 15.0m 29.837610006332397s.\n",
      "\n",
      "Processing question topic queries ['data/hi/parsed/cord19.topics.questions.hi2eng.gt.emb.synset.translit.txt', '4gram/ranked/4gram_rankedlist_hi2en_gt_emb_synset_translit_questions.txt']...\n",
      "TF/IDF weights for each query term in first query\n",
      "  orig: 12.285383200597506\n",
      "  rigi: 12.20868289238081\n",
      "  igin: 12.349307810056732\n",
      "  id19: 7.302551198823754\n",
      "  kovi: 10.364625258503148\n",
      "  ovid: 0.9389535536710554\n",
      "  gins: 6.509047897225928\n",
      "  gina: 4.49315544883056\n",
      "  inat: 2.3479706411004333\n",
      "  nate: 2.99541150869659\n",
      "  ated: 0.8903178991737719\n",
      "  wond: 9.737179426460479\n",
      "  onde: 5.253939460435512\n",
      "  nder: 2.017607254810786\n",
      "  deri: 3.729851754559513\n",
      "  erin: 2.9552928507348284\n",
      "  ring: 1.9937575073012348\n",
      "All queries scored in: 10m 13.46s\t\n",
      "Question topic queries processing complete in 16.0m 51.06861615180969s.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for qfile in question_files:\n",
    "    print(f\"Processing question topic queries {qfile}...\")\n",
    "    start = time.time()\n",
    "    scores_by_qid = get_scores_by_qid(corpus, inverted_index, qfile[0])\n",
    "    rank_top_100_scores(scores_by_qid, qfile[1])\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    mins = elapsed // 60\n",
    "    secs = elapsed - (60*mins)\n",
    "    print(f\"Question topic queries processing complete in {mins}m {secs}s.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
