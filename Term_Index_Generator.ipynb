{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Index Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pickle\n",
    "import numpy\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import os\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import import_ipynb\n",
    "import clir_files as files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PICKLE_READ_WRITE = False\n",
    "global PUNCTUATION\n",
    "global NUM_BYTES\n",
    "global BYTE_ORDER\n",
    "global SIGNED \n",
    "global STOP_WORDS\n",
    "global RUN_TESTS\n",
    "RUN_TESTS = True\n",
    "TERM_COUNT = 0\n",
    "SIGNED = False\n",
    "BYTE_ORDER = \"big\"\n",
    "NUM_BYTES = 4\n",
    "\n",
    "PUNCTUATION = set(string.punctuation)\n",
    "\n",
    "EN_STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "def increment_term_count():\n",
    "    global TERM_COUNT\n",
    "    TERM_COUNT = TERM_COUNT + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus():\n",
    "    \"\"\"\n",
    "    This object represents the corpus.\n",
    "    Attributes:\n",
    "        docs_processed (int): Number of documents processed.\n",
    "        lexicon (dict): Dictionaryh object representing lexicon\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.docs_processed = 0\n",
    "        self.lexicon = {}\n",
    "        self.collection_freq = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Posting():\n",
    "    \"\"\"\n",
    "    This object represents one posting in a postings list.\n",
    "    Attributes:\n",
    "        doc_id (int): Document's id.\n",
    "        term_freq (int): Number of times the term occurs in the document.\n",
    "    \"\"\"\n",
    "    def __init__(self, doc_id, term_freq):\n",
    "        self.doc_id = doc_id\n",
    "        self.term_freq = term_freq\n",
    "    def __str__(self):\n",
    "        return f\"({self.doc_id},{self.term_freq})\"\n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Term():\n",
    "    \"\"\"\n",
    "    This object represents a term.\n",
    "    Attributes:\n",
    "        offset (int): The beginning offset for the postings list in a binary file.\n",
    "        id (str): The term id for this object\n",
    "        doc_freq (int): Number of documents containing this term.\n",
    "    \"\"\"\n",
    "    def __init__(self, term_id):\n",
    "        self.offset = 0\n",
    "        self.id = term_id\n",
    "        self.doc_freq = 0\n",
    "        self.idf = -1\n",
    "    def __str__(self):\n",
    "        return f\"doc_freq: {self.doc_freq}; offset: {self.offset}; id: {self.id}; idf: {self.idf}\"\n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File IO Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    \"\"\"\n",
    "    Reads file.\n",
    "    Parameters:\n",
    "        file_name (str): The filename for file to be read.\n",
    "    Returns:\n",
    "        content (str): contents of the file.\n",
    "    \"\"\"\n",
    "    with open(file_name) as file:\n",
    "        content = file.read()\n",
    "        return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_lexicon_to_file(lexicon, filename):\n",
    "    \"\"\"\n",
    "    Write lexicon to a file.\n",
    "    Parameters:\n",
    "        lexicon (dict{str, Term}): Object representing the lexicon.\n",
    "        filename (str): Filename for the file created.\n",
    "    Returns:\n",
    "        N/A\n",
    "    \"\"\"\n",
    "    pickle.dump(lexicon, open(filename, \"wb\" ))\n",
    "    \n",
    "def read_lexicon_from_file(filename):\n",
    "    \"\"\"\n",
    "    Read lexicon from file.\n",
    "    Parameters:\n",
    "        filename (str): Filename for the file read.\n",
    "    Returns:\n",
    "        Lexicon object loaded from file\n",
    "    \"\"\"\n",
    "    return pickle.load(open(filename, \"rb\" )) \n",
    "\n",
    "if TEST_PICKLE_READ_WRITE:\n",
    "    test_obj = CorpusStats()\n",
    "    test_obj.num_paras = 7\n",
    "    write_corpus_to_file(test_obj, \"read_write_test.pk\")\n",
    "    read_test = read_corpus_from_file(\"read_write_test.pk\")\n",
    "    assert read_test.num_paras == 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_file_write(lexicon, postings, bin_file_name):\n",
    "    \"\"\"\n",
    "    Write postings list to two binary files and use offset to track the beginning of the binary file.\n",
    "    Attributes:\n",
    "        lexicon (dict{str: Term}): A dict representing the lexicon.\n",
    "        postings (dict{str: list[Posting]}): A list of Posting objects.\n",
    "        bin_file_name (str): Filename for binary file containing interleaving doc_ids and term_freqs as 32-bit ints.\n",
    "    Returns:\n",
    "        N/A\n",
    "    \"\"\"\n",
    "    bin_file = open(bin_file_name, \"wb\")\n",
    "    \n",
    "    assert bin_file.tell() == 0\n",
    "    offset = 0\n",
    "    for key, val in lexicon.items():\n",
    "        for posting in postings[val.id]:\n",
    "            bin_file.write((int(posting.doc_id)).to_bytes(NUM_BYTES, byteorder=BYTE_ORDER, signed=SIGNED))\n",
    "            bin_file.write((int(posting.term_freq)).to_bytes(NUM_BYTES, byteorder=BYTE_ORDER, signed=SIGNED))\n",
    "        val.offset = offset\n",
    "        offset = bin_file.tell()\n",
    "    \n",
    "    bin_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_top_100_scores(scores, filename):\n",
    "    file = open(filename, \"a\")\n",
    "    for qid, query_scores in scores.items():\n",
    "        try:\n",
    "            top_100 = dict(sorted(query_scores.items(), key=lambda k:k[1], reverse = True)[0:100])\n",
    "        except IndexError:\n",
    "            top_100 = dict(sorted(query_scores.items(), key=lambda k:k[1], reverse = True))\n",
    "        for rank, (doc_id, doc_score) in enumerate(top_100.items(), 1):\n",
    "            file.write(f\"{qid} Q0 {doc_id} {rank} {doc_score:.6f} skasim3\\n\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_punctuation(doc):\n",
    "    \"\"\"\n",
    "    Strips punctuation from a document.\n",
    "    Parameters:\n",
    "        doc (str): A string representing the document.\n",
    "    Returns:\n",
    "        stripped (str): A string that has been stripped of punctuation.\n",
    "        \n",
    "    \"\"\"    \n",
    "    separated = \"\"\n",
    "    for ch in doc:\n",
    "        if ch in PUNCTUATION and ch != \".\":\n",
    "            separated = f\"{separated} {ch} \"\n",
    "        else:\n",
    "            separated = f\"{separated}{ch}\"\n",
    "    return separated.strip()\n",
    "\n",
    "if RUN_TESTS:\n",
    "    assert separate_punctuation(\"That's my mother-in-law\") == \"That ' s my mother - in - law\"\n",
    "    assert separate_punctuation(\"\"\"NAC has developed a National HIV/AIDS/STI/TB Intervention Strategic Plan (2002-2005) that aims to reduce the HIV prevalence rate among Zambians from 19.3% to 11.7% and improve the health status of people living with HIV/AIDS by 2005.\"\"\") == \"\"\"NAC has developed a National HIV / AIDS / STI / TB Intervention Strategic Plan  ( 2002 - 2005 )  that aims to reduce the HIV prevalence rate among Zambians from 19.3 %  to 11.7 %  and improve the health status of people living with HIV / AIDS by 2005.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_doc(doc):\n",
    "    \"\"\"\n",
    "    Removes new lines, multiple periods (e.g., mercedes...awesome), punctuation, multiple spaces (e.g., mercedes    awesome), and lowers the case of the provided document.\n",
    "    Parameters:\n",
    "        doc (str): A string representing the document.\n",
    "    Returns:\n",
    "        multi_space_stripped_doc (str): A string stripped of new lines, multiple periods, punctuation, multiple spaces with case lowered.\n",
    "    \"\"\"\n",
    "    stripped_doc = doc.strip(\"\\n\").lower()\n",
    "    multi_period_stripped_doc = re.sub(\"\\.\\.+\", \" \", stripped_doc)\n",
    "    punctuation_separated_doc = separate_punctuation(multi_period_stripped_doc)\n",
    "    multi_space_stripped_doc = re.sub(\" +\", \" \", punctuation_separated_doc)\n",
    "    new_line_stripped_doc = re.sub(\"\\n\",\" \", multi_space_stripped_doc)\n",
    "    return new_line_stripped_doc.strip()\n",
    "\n",
    "if RUN_TESTS:\n",
    "    assert strip_doc(\"To Be? Or* #not To +% be T&HAT I^s...the 99$ question!\") == \"to be ? or * # not to + % be t & hat i ^ s the 99 $ question !\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_tokens(doc):\n",
    "    \"\"\"\n",
    "    Splits document based on space character into a list of words/terms and removes any digits so that they are not counted as a word, e.g. \"20\"\n",
    "    Parameters:\n",
    "        doc (str): A string.\n",
    "    Returns:\n",
    "        terms (list[str]): A list of strings, which are the individual word/term.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(doc)\n",
    "    return [t for t in tokens if t not in PUNCTUATION and t not in EN_STOP_WORDS]\n",
    "if RUN_TESTS:\n",
    "    assert split_into_tokens(\"to be or not to be that is the question\") == ['question']\n",
    "    assert split_into_tokens(\"99 bottles on the wall\") == ['99', 'bottles', 'wall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_and_remove_doc_id(doc, is_doc=True):\n",
    "    \"\"\"\n",
    "    Utilize regex to capture document id and remove document id from the text, so that it is not processed.\n",
    "    Parameters:\n",
    "        doc (str): A string.\n",
    "    Returns:\n",
    "        doc_id (str): The document id.\n",
    "        modified_doc (str): Document with the pattern <p id = xx> removed.\n",
    "    \"\"\"\n",
    "    if is_doc:\n",
    "        pattern = r\"< p id = \\d+ >\"\n",
    "    else:\n",
    "        pattern = r\"< q id = \\d+ >\"\n",
    "    doc_id = re.findall(r\"\\d+\", doc)\n",
    "    modified_doc = re.sub(pattern, \"\", doc).strip(\"\\n\")\n",
    "    return doc_id[0], (re.sub(\" +\", \" \", modified_doc)).strip()\n",
    "\n",
    "if RUN_TESTS:\n",
    "    assert capture_and_remove_doc_id(\"99 bottles on the wall\")[1] == \"99 bottles on the wall\"\n",
    "    assert capture_and_remove_doc_id(\"< p id = 2 > 99 bottles on the wall\")[0][0] == \"2\"\n",
    "    assert capture_and_remove_doc_id(\"< p id = 27 > 99 bottles on the wall\")[1] == \"99 bottles on the wall\"\n",
    "    assert capture_and_remove_doc_id(\"< q id = 1 > 99 bottles on the wall\", is_doc=False)[1] == \"99 bottles on the wall\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicon Enrichment Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_lexicon(corpus, postings, words, doc_id):\n",
    "    \"\"\"\n",
    "    Enrich lexicon with list of words and document_id provided:\n",
    "        * If word is not in the lexicon, the word is added to lexicon, a Posting object is instantiated, and doc_freq is incremented by 1.\n",
    "        * If word is in lexicon, a check is done to see if the word reoccurs in the last indexed document:\n",
    "            - If the word reoccurs in the same document, the previously saved term_freq count for the document is incremented by 1.\n",
    "            - If the word does not reoccur in the same document, but is in the lexicon, then a new Posting object is created and doc_freq is incremented by 1.\n",
    "    Parameters:\n",
    "        lexicon (Corpus): An object representing the corpus.\n",
    "        postings (dict{str: list[Posting]}): A list of Posting objects.\n",
    "        words (list[str]): A list of words.\n",
    "        doc_id: The document id of the document from, which words was retrieved.\n",
    "    Returns:\n",
    "        lexicon (dict{str: Term}): Updated lexicon.\n",
    "        postings (dict{str: list[Posting]}): Updated postings.\n",
    "    \"\"\"\n",
    "\n",
    "    for word in words:\n",
    "        corpus.collection_freq += 1\n",
    "        if word not in corpus.lexicon:\n",
    "            posting = Posting(doc_id, 1)\n",
    "            increment_term_count()\n",
    "            term = Term(TERM_COUNT)\n",
    "            term.doc_freq = 1\n",
    "            postings[term.id] = [posting]\n",
    "            corpus.lexicon[word] = term\n",
    "        else:\n",
    "            prev_indexed_term_id = corpus.lexicon[word].id\n",
    "            last_doc_id = postings[prev_indexed_term_id][-1].doc_id\n",
    "            if last_doc_id == doc_id:\n",
    "                postings[prev_indexed_term_id][-1].term_freq += 1\n",
    "            else:\n",
    "                posting = Posting(doc_id, 1)\n",
    "                corpus.lexicon[word].doc_freq += 1\n",
    "                term_id = corpus.lexicon[word].id\n",
    "                postings[term_id].append(posting)\n",
    "\n",
    "    return corpus   \n",
    "\n",
    "if RUN_TESTS:\n",
    "    test_corpus = Corpus()\n",
    "    test_postings = {}\n",
    "    test_words = [\"you\", \"are\", \"you\"]\n",
    "    test_enriched = enrich_lexicon(test_corpus, test_postings, test_words, 77)\n",
    "    assert test_enriched.lexicon[\"you\"].doc_freq == 1\n",
    "    t1 = test_enriched.lexicon[\"you\"].id\n",
    "    assert str(test_postings[t1][0]) == \"(77,2)\"\n",
    "    assert test_enriched.lexicon[\"are\"].doc_freq == 1\n",
    "    t2 = test_enriched.lexicon[\"are\"].id\n",
    "    assert str(test_postings[t2][0]) == \"(77,1)\"\n",
    "    test_new_words = [\"you\", \"thirsty\"]\n",
    "    test_enriched_update = enrich_lexicon(test_corpus, test_postings, test_new_words, 32)\n",
    "    assert test_enriched_update.lexicon[\"you\"].doc_freq == 2\n",
    "    assert str(test_postings[t1][0]) == \"(77,2)\"\n",
    "    assert str(test_postings[t1][1]) == \"(32,1)\"\n",
    "    assert test_enriched_update.lexicon[\"thirsty\"].doc_freq == 1\n",
    "    t3 = test_enriched.lexicon[\"thirsty\"].id\n",
    "    assert str(test_postings[t3][0]) == \"(32,1)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_postings(postings):\n",
    "    \"\"\"\n",
    "    Sort postings list by doc_id.\n",
    "    Attributes:\n",
    "        postings(list[Posting]): A list of Posting objects.\n",
    "    Returns:\n",
    "        postings(list[Posting])\n",
    "    \"\"\"\n",
    "    for k, post in postings.items():\n",
    "        post.sort(key=lambda k:k.doc_id, reverse=False)\n",
    "    return postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_terms(lexicon):\n",
    "    \"\"\"\n",
    "    Sort terms in a lexicon in alphabetical order.\n",
    "    Attributes:\n",
    "        lexicon(dict{str: Term}). A dictionary representing the lexicon.\n",
    "    Returns:\n",
    "        sorted lexicon\n",
    "    \"\"\"\n",
    "    return dict(sorted(lexicon.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_postings_list_for_term(word, lexicon, binary_file):\n",
    "    \"\"\"\n",
    "    Retrieves posting list for a term.\n",
    "    Attributes:\n",
    "        word (str): The term being retrieved\n",
    "        lexicon (dict{str: Term}): A dict representing the lexicon.\n",
    "        doc_id_file (str): Filename for binary file containing doc_ids as 32-bit ints.\n",
    "        term_freq_file (str): Filename for binary file containing term_freqs as 32-bit ints.\n",
    "    Returns:\n",
    "        postings_list (list[Posting]): A list of Posting objects for a term.\n",
    "    \"\"\"\n",
    "    binary_file = open(binary_file, \"rb\")\n",
    "    \n",
    "    try:\n",
    "        postings_list = []\n",
    "        term = lexicon[word]\n",
    "        offset = term.offset\n",
    "        binary_file.seek(int(term.offset))\n",
    "        for _ in range(0, term.doc_freq):\n",
    "            bytes_read_doc_id = binary_file.read(NUM_BYTES)\n",
    "            bytes_read_term_freq = binary_file.read(NUM_BYTES)\n",
    "            doc_id = int.from_bytes(bytes_read_doc_id, byteorder=BYTE_ORDER,signed=SIGNED)\n",
    "            term_freq = int.from_bytes(bytes_read_term_freq, byteorder=BYTE_ORDER,signed=SIGNED)\n",
    "            posting = Posting(doc_id, term_freq)\n",
    "            postings_list.append(posting)\n",
    "        return postings_list\n",
    "    except KeyError as e:\n",
    "        return []\n",
    "    \n",
    "    binary_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Enrichment Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query_file(filename):\n",
    "    \"\"\"\n",
    "    Given the query filename, process and tokenize query terms in the exact same manner as the documents.\n",
    "    Attributes:\n",
    "        filename (str): Filename for the query file.\n",
    "    Returns:\n",
    "        queries (dict(int, list[str])): Return a dict for which the key is an int value representing query_id and \n",
    "        value is a list of strings representing the terms for that query.\n",
    "    \"\"\"\n",
    "    content = read_file(filename)\n",
    "    content = re.split(\"</Q>\", content)\n",
    "    queries_processed = 0\n",
    "    queries = {}\n",
    "    for query in content:\n",
    "        validation_check = re.findall(r\"\\d+\", query)\n",
    "        try:\n",
    "            validation_check[0]\n",
    "            clean_query = strip_doc(query)\n",
    "            query_id, modified_query = capture_and_remove_doc_id(clean_query, is_doc=False)\n",
    "            query_terms = split_into_tokens(modified_query)\n",
    "            queries[query_id] = query_terms\n",
    "            queries_processed += 1\n",
    "        except IndexError:\n",
    "            pass\n",
    "    return queries\n",
    "\n",
    "if RUN_TESTS:\n",
    "    assert str(process_query_file(\"data/animal.topics.txt\")) == \"{'1': ['bird', 'cat', 'dog']}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_TESTS:\n",
    "    # For testing\n",
    "    test_binary_file = \"TEST_binary.bin\"\n",
    "    tc_lexicon = {}\n",
    "    red1 = Posting(3, 7)\n",
    "    red2 = Posting(5, 1)\n",
    "    red3 = Posting(7, 9)\n",
    "    redterm = Term(1)\n",
    "    redterm.offset = 16\n",
    "    redterm.doc_freq = 3\n",
    "    tc_lexicon[\"red\"] = redterm\n",
    "    blue1 = Posting(3, 5)\n",
    "    blue2 = Posting(7, 2)\n",
    "    blueterm = Term(2)\n",
    "    blueterm.offset = 0\n",
    "    blueterm.doc_freq = 2\n",
    "    tc_lexicon[\"blue\"] = blueterm\n",
    "    tc_corpus = Corpus()\n",
    "    tc_corpus.lexicon = tc_lexicon\n",
    "    tc_corpus.docs_processed = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_idf_for_term(corpus, term):\n",
    "    \"\"\"\n",
    "    Calculate IDF(t) = log_2(N/df(t)) for a term.\n",
    "    Attributes:\n",
    "        corpus (Corpus): The Corpus object.\n",
    "        term (str): String representing one term.\n",
    "    Returns:\n",
    "        float representing IDF for a term in lexicon.\n",
    "    \"\"\"\n",
    "    N = corpus.docs_processed\n",
    "    df_t = corpus.lexicon[term].doc_freq\n",
    "    return numpy.log2(N/df_t)\n",
    "\n",
    "if RUN_TESTS:\n",
    "    assert round(calc_idf_for_term(tc_corpus, \"red\"), 6) == .736966\n",
    "    assert round(calc_idf_for_term(tc_corpus, \"blue\"), 6) == 1.321928"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_idf_values_in_corpus(corpus):\n",
    "    \"\"\"\n",
    "    Given a corpus, loop through each term in the lexicon, calculate the term's IDF and update corpus.lexicon[term].idf\n",
    "    field.\n",
    "    Attributes:\n",
    "        corpus (Corpus): A corpus object.\n",
    "    Returns:\n",
    "        corpus (Corpus): The corpus.lexicon with the updated IDF values.\n",
    "    \"\"\"\n",
    "    for term in corpus.lexicon:\n",
    "        corpus.lexicon[term].idf = calc_idf_for_term(corpus, term)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_doc_vector_lengths(corpus, binary_file):\n",
    "    \"\"\"\n",
    "    Calculate vector lengths for a corpus. Since inverted index/postings list for a term is stored in a binary file,\n",
    "    read binary file and return posting list for a given term.\n",
    "    \"\"\"\n",
    "    doclens = defaultdict(float)\n",
    "    for term in corpus.lexicon:\n",
    "        postings_list = get_postings_list_for_term(term, corpus.lexicon, binary_file)\n",
    "        for posting in postings_list:\n",
    "            weight = int(posting.term_freq) * calc_idf_for_term(corpus, term)\n",
    "            doclens[int(posting.doc_id)] += weight * weight\n",
    "    for doc_id, summ in doclens.items():\n",
    "        doclens[doc_id] = numpy.sqrt(summ)\n",
    "    return dict(sorted(doclens.items()))\n",
    "\n",
    "\n",
    "if RUN_TESTS:\n",
    "    tc_doclens = calc_doc_vector_lengths(tc_corpus, test_binary_file)\n",
    "    assert round(tc_doclens[3], 4) == 8.3845\n",
    "    assert round(tc_doclens[5], 4) == .7370\n",
    "    assert round(tc_doclens[7], 4) == 7.1402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_query_tfs(corpus, query_terms):\n",
    "    \"\"\"\n",
    "    Calcuate term frequencies of a an array of query terms. If that term exists in the lexicon. If it doesn't exist in the lexicon\n",
    "    then ignore the term.\n",
    "    Attributes:\n",
    "        corpus (Corpus): A Corpus object\n",
    "        query_terms (list[str]): A list of string objects representing a query term.\n",
    "    Returns:\n",
    "        term_tfs (dict[str, int]): A dict object representing the term (str) and its term frequency (int).\n",
    "    \"\"\"\n",
    "    term_tfs = {}\n",
    "    for term in query_terms:\n",
    "        if term in corpus.lexicon:\n",
    "            if term in term_tfs:\n",
    "                term_tfs[term] += 1\n",
    "            else:\n",
    "                term_tfs[term] = 1\n",
    "    return term_tfs\n",
    "\n",
    "if RUN_TESTS:\n",
    "    qtc = \"want blue blue blue red\"\n",
    "    qtc = qtc.split()\n",
    "    qtc_tfs = calc_query_tfs(tc_corpus, qtc)\n",
    "    assert str(qtc_tfs) == \"{'blue': 3, 'red': 1}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_query_vector_length(corpus, query_tfs):\n",
    "    \"\"\"\n",
    "    Given the corpus and the query term frequencies, calculate the query vector length.\n",
    "    Attributes:\n",
    "        corpus (Corpus): A Corpus object.\n",
    "        query_tfs (dict): dict[str, int]): A dict object representing the term (str) and its term frequency (int).\n",
    "    Returns:\n",
    "        float representing the query vector length.\n",
    "    \"\"\"\n",
    "    sum_of_sqrs = 0\n",
    "    for term, tf_q in query_tfs.items():\n",
    "        weight = tf_q * calc_idf_for_term(corpus, term)\n",
    "        sum_of_sqrs += weight * weight\n",
    "    return numpy.sqrt(sum_of_sqrs)\n",
    "\n",
    "if RUN_TESTS:\n",
    "    assert round(calc_query_vector_length(tc_corpus, qtc_tfs), 4) == 4.0337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_query_tfs_veclens_by_qid(corpus, queries_dict):\n",
    "    \"\"\"\n",
    "    Given the corpus and the queries_dict that has query_id as key and list of query terms as value, utilize the IDF\n",
    "    value for a term in the corpus to calculate the query term frequences.\n",
    "    Attributes:\n",
    "        corpus (Corpus): A Corpus object.\n",
    "        queries (dict(int, list[str])): Return a dict for which the key is an int value representing query_id and \n",
    "        value is a list of strings representing the terms for that query.\n",
    "    Returns:\n",
    "        q_tfs (dict[int, list[int, flot]]): A dict with query_id as key and value is a list of len 2, with query_tf\n",
    "        at index 0 and query_vectorlength at index 1.\n",
    "    \"\"\"\n",
    "    q_tfs_veclens = {}\n",
    "    for qid, qterms in queries_dict.items():\n",
    "        q_tf = calc_query_tfs(corpus, qterms)\n",
    "        q_veclen = calc_query_vector_length(corpus, q_tf)\n",
    "        q_tfs_veclens[qid] = [q_tf, q_veclen] # {query id: [query term frequency, query veclen]}\n",
    "    return q_tfs_veclens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_documents_for_cos_sim(corpus, query_tfs_veclens_by_qid, doclens, binary_file):\n",
    "    scores_by_qid = {}\n",
    "    for qid, qterm in query_tfs_veclens_by_qid.items():\n",
    "        score = defaultdict(float)\n",
    "        for term in qterm[0]:\n",
    "            try:\n",
    "                postings_list = get_postings_list_for_term(term, corpus.lexicon, binary_file)\n",
    "                for posting in postings_list:\n",
    "                    score[posting.doc_id] += query_tfs_veclens_by_qid[qid][0][term] * corpus.lexicon[term].idf * posting.term_freq * corpus.lexicon[term].idf\n",
    "            except KeyError:\n",
    "                pass\n",
    "        for doc_id, value in score.items():\n",
    "            score[doc_id] /= (doclens[float(doc_id)] * query_tfs_veclens_by_qid[qid][1]) if (doclens[float(doc_id)] * query_tfs_veclens_by_qid[qid][1]) !=0 else .00000000000000001 # account for div by 0\n",
    "        scores_by_qid[qid] = score\n",
    "    return scores_by_qid\n",
    "\n",
    "if RUN_TESTS:\n",
    "    # for correctness of cosine scores, check assertions below\n",
    "    tc_corpus = populate_idf_values_in_corpus(tc_corpus)\n",
    "    assert round(tc_corpus.lexicon[\"red\"].idf, 6) == .736966\n",
    "    assert round(tc_corpus.lexicon[\"blue\"].idf, 6) == 1.321928\n",
    "    tc_doclens = calc_doc_vector_lengths(tc_corpus, test_binary_file)\n",
    "    assert round(tc_doclens[3], 4) == 8.3845\n",
    "    assert round(tc_doclens[5], 4) == .7370\n",
    "    assert round(tc_doclens[7], 4) == 7.1402\n",
    "    qtc2 = \"want blue blue blue red\"\n",
    "    qtc2 = qtc2.split()\n",
    "    qtc_dict2 = {\"1\":qtc2}\n",
    "    tc_query_veclens_by_qid = calculate_query_tfs_veclens_by_qid(tc_corpus, qtc_dict2)\n",
    "    assert str(tc_query_veclens_by_qid[\"1\"][0]) == \"{'blue': 3, 'red': 1}\"\n",
    "    assert round(tc_query_veclens_by_qid[\"1\"][1], 4) == 4.0337\n",
    "    tc_scores = score_documents_for_cos_sim(tc_corpus, tc_query_veclens_by_qid, tc_doclens, test_binary_file)\n",
    "    assert round(tc_scores[\"1\"][3], 4) == .8875\n",
    "    assert round(tc_scores[\"1\"][5], 4) == .1827\n",
    "    assert round(tc_scores[\"1\"][7], 4) == .5338"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Lexicon and Postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inverted_index(filename, binary_file_name, lexicon_file_name):\n",
    "    \"\"\"\n",
    "    Create binary inverted index for file read. Split file based on \"</P>\" into individual documents. \n",
    "    Conduct a validation check to ensure document can be parsed. Enrich document. Create two binary files\n",
    "    Parameters:\n",
    "        filename: Filename representing the file consumed.\n",
    "        binary_file_name: Name of binary file to which interleaving  32-bit int doc_ids and term_freqs are written to.\n",
    "        lexicon_file_name: Name of .pk file to which the lexicon object is written to.\n",
    "    Returns:\n",
    "        corpus (Corpus): Object represents corpus generated from the file consumed.\n",
    "    \"\"\"\n",
    "    corpus = Corpus()\n",
    "    postings = {}\n",
    "    content = read_file(filename)\n",
    "    content = re.split(\"</P>\", content)\n",
    "    docs_processed = 0\n",
    "    for doc in content:\n",
    "        validation_check = re.findall(r\"\\d+\", doc)\n",
    "        try: \n",
    "            validation_check[0]\n",
    "            clean_doc = strip_doc(doc)\n",
    "            doc_id, modified_doc = capture_and_remove_doc_id(clean_doc)\n",
    "            terms = split_into_tokens(modified_doc)\n",
    "            enrich_lexicon(corpus, postings, terms, doc_id) \n",
    "            docs_processed += 1\n",
    "        except IndexError:\n",
    "            pass\n",
    "    sort_postings(postings)\n",
    "    sorted_lexicon = sort_terms(corpus.lexicon)\n",
    "    corpus.lexicon = sorted_lexicon\n",
    "    corpus.docs_processed = docs_processed\n",
    "    # write postings dict to .bin file\n",
    "    binary_file_write(corpus.lexicon, postings, binary_file_name)\n",
    "    print(f\"Total docs processed: {corpus.docs_processed}\")\n",
    "    populate_idf_values_in_corpus(corpus)\n",
    "    # write corpus to .pk file\n",
    "    write_lexicon_to_file(corpus, lexicon_file_name)\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Inverted Index - Write Binary File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs processed: 8\n",
      "Run time to create inverted index: 0m 0.01s\n"
     ]
    }
   ],
   "source": [
    "inverted_index = \"term/term_inverted_index.bin\"\n",
    "lexicon_file = \"term/term_corpus.pk\"\n",
    "ranked_a = \"term/term_rankedlist_a.txt\"\n",
    "ranked_b = \"term/term_rankedlist_b.txt\"\n",
    "\n",
    "text_file = \"data/animal.txt\"\n",
    "cord19_topics_keyword = \"data/animal.topics.txt\"\n",
    "cord19_topics_questions = \"data/animal.topics.txt\"\n",
    "\n",
    "# text_file = \"data/cord19.txt\"\n",
    "# cord19_topics_keyword = \"data/cord19.topics.keyword.txt\"\n",
    "# cord19_topics_questions = \"data/cord19.topics.question.txt\"\n",
    "\n",
    "start = time.time()\n",
    "# Write .bin and .pk files\n",
    "corpus = create_inverted_index(text_file, inverted_index, lexicon_file)\n",
    "end = time.time()\n",
    "\n",
    "elapsed = end - start\n",
    "mins = elapsed // 60\n",
    "secs = elapsed - (60*mins)\n",
    "print(f\"Run time to create inverted index: {round(mins)}m {round(secs, 2)}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_by_qid(corpus, binary_file, query_file):\n",
    "    doclens = calc_doc_vector_lengths(corpus, inverted_index)\n",
    "    queries = process_query_file(query_file)\n",
    "    \n",
    "    tf_idfs1 = calc_query_tfs(corpus, queries[\"1\"])\n",
    "    print(\"TF/IDF weights for each query term in first query\")\n",
    "    for t, qtf in tf_idfs1.items():\n",
    "        print(f\"  {t}: {qtf * calc_idf_for_term(corpus, t)}\")\n",
    "    \n",
    "    query_tfs_veclens_by_qid = calculate_query_tfs_veclens_by_qid(corpus, queries)\n",
    "    \n",
    "    start = time.time()\n",
    "    scores_by_qid = score_documents_for_cos_sim(corpus, query_tfs_veclens_by_qid, doclens, binary_file)\n",
    "    end = time.time()\n",
    "    \n",
    "    elapsed = end - start\n",
    "    mins = elapsed // 60\n",
    "    secs = elapsed - (60*mins)\n",
    "    print(f\"All queries scored in: {round(mins)}m {round(secs, 2)}s\\t\")\n",
    "    return scores_by_qid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Ranked List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing keyword topic queries...\n",
      "TF/IDF weights for each query term in first query\n",
      "  bird: 0.0\n",
      "  cat: 3.0\n",
      "  dog: 1.0\n",
      "All queries scored in: 0m 0.0s\t\n",
      "Keyword topic queries processing complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing keyword topic queries...\")\n",
    "scores_by_qid = get_scores_by_qid(corpus, inverted_index, cord19_topics_keyword)\n",
    "rank_top_100_scores(scores_by_qid, ranked_a)\n",
    "print(\"Keyword topic queries processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing question topic queries...\n",
      "TF/IDF weights for each query term in first query\n",
      "  bird: 0.0\n",
      "  cat: 3.0\n",
      "  dog: 1.0\n",
      "All queries scored in: 0m 0.0s\t\n",
      "Question topic queries processing complete.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nProcessing question topic queries...\")\n",
    "scores_by_qid = get_scores_by_qid(corpus, inverted_index, cord19_topics_questions)\n",
    "rank_top_100_scores(scores_by_qid, ranked_b)\n",
    "print(\"Question topic queries processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of docs indexed: 8\n",
      "Collection size: 50\n",
      "Vocab size: 7\n",
      "inverted_index.bin size: 0.0 MB\n",
      "lexicon.pk size: 0.0 MB\n"
     ]
    }
   ],
   "source": [
    "corpus = read_lexicon_from_file(lexicon_file)\n",
    "lexicon = corpus.lexicon\n",
    "print(f\"\\nNumber of docs indexed: {corpus.docs_processed}\")\n",
    "collection_size = corpus.collection_freq\n",
    "vocab_size = len(lexicon)\n",
    "print(f\"Collection size: {collection_size}\")\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "bin_file_size = os.path.getsize(inverted_index)\n",
    "lex_size = os.path.getsize(lexicon_file)\n",
    "print(f\"inverted_index.bin size: {round(bin_file_size/(1024*1024.0), 2)} MB\")\n",
    "print(f\"lexicon.pk size: {round(lex_size/(1024*1024.0), 2)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aardvark': doc_freq: 4; offset: 0; id: 6; idf: 1.0, 'bid': doc_freq: 1; offset: 32; id: 10; idf: 3.0, 'bird': doc_freq: 8; offset: 40; id: 4; idf: 0.0, 'cat': doc_freq: 1; offset: 104; id: 7; idf: 3.0, 'dog': doc_freq: 4; offset: 112; id: 5; idf: 1.0, 'egret': doc_freq: 2; offset: 144; id: 8; idf: 2.0, 'fish': doc_freq: 2; offset: 160; id: 9; idf: 2.0}\n"
     ]
    }
   ],
   "source": [
    "# print(read_lexicon_from_file(lexicon_file).lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIR Query Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = \"term/term_inverted_index.bin\"\n",
    "lexicon_file = \"term/term_corpus.pk\"\n",
    "corpus = read_lexicon_from_file(lexicon_file)\n",
    "lexicon = corpus.lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out keywords\n",
    "ranked_en_base_keywords = \"term/ranked/term_rankedlist_en_base_keywords.txt\"\n",
    "ranked_hi2en_gt_keywords = \"term/ranked/term_rankedlist_hi2en_gt_keywords.txt\"\n",
    "ranked_hi2en_emb_keywords = \"term/ranked/term_rankedlist_hi2en_emb_keywords.txt\"\n",
    "ranked_hi2en_emb_synset_keywords = \"term/ranked/term_rankedlist_hi2en_emb_synset_keywords.txt\"\n",
    "ranked_hi2en_emb_no_oov_keywords = \"term/ranked/term_rankedlist_hi2en_emb_no_oov_keywords.txt\"\n",
    "ranked_hi2en_gt_emb_synset_translit_keywords = \"term/ranked/term_rankedlist_hi2en_gt_emb_synset_translit_keywords.txt\"\n",
    "\n",
    "# out questions\n",
    "ranked_en_base_questions = \"term/ranked/term_rankedlist_en_base_questions.txt\"\n",
    "ranked_hi2en_gt_questions = \"term/ranked/term_rankedlist_hi2en_gt_questions.txt\"\n",
    "ranked_hi2en_emb_questions = \"term/ranked/term_rankedlist_hi2en_emb_questions.txt\"\n",
    "ranked_hi2en_emb_synset_questions = \"term/ranked/term_rankedlist_hi2en_emb_synset_questions.txt\"\n",
    "ranked_hi2en_emb_no_oov_questions = \"term/ranked/term_rankedlist_hi2en_emb_no_oov_questions.txt\"\n",
    "ranked_hi2en_gt_emb_synset_translit_questions = \"term/ranked/term_rankedlist_hi2en_gt_emb_synset_translit_questions.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_files = [\n",
    "    [files.cord19_topics_keyword, ranked_en_base_keywords],\n",
    "    [files.cord19_topics_keyword_HIN2ENG_GT, ranked_hi2en_gt_keywords],\n",
    "    [files.cord19_topics_keyword_HIN2ENG_EMB, ranked_hi2en_emb_keywords],\n",
    "    [files.cord19_topics_keyword_HIN2ENG_EMB_SYNSET, ranked_hi2en_emb_synset_keywords],\n",
    "    [files.cord19_topics_keyword_HIN2ENG_EMB_NO_OOV, ranked_hi2en_emb_no_oov_keywords],\n",
    "    [files.cord19_topics_keyword_HIN2ENG_GT_EMB_SYNSET_TRANSLIT, ranked_hi2en_gt_emb_synset_translit_keywords]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_files = [\n",
    "    [files.cord19_topics_questions, ranked_en_base_questions],\n",
    "    [files.cord19_topics_questions_HIN2ENG_GT, ranked_hi2en_gt_questions],\n",
    "    [files.cord19_topics_questions_HIN2ENG_EMB, ranked_hi2en_emb_questions],\n",
    "    [files.cord19_topics_questions_HIN2ENG_EMB_SYNSET, ranked_hi2en_emb_synset_questions],\n",
    "    [files.cord19_topics_questions_HIN2ENG_EMB_NO_OOV, ranked_hi2en_emb_no_oov_questions],\n",
    "    [files.cord19_topics_questions_HIN2ENG_GT_EMB_SYNSET_TRANSLIT, ranked_hi2en_gt_emb_synset_translit_questions]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hi2en ranked list keywords for terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing keyword topic queries ['data/cord19.topics.keyword.txt', 'term/ranked/term_rankedlist_en_base_keywords.txt']...\n",
      "TF/IDF weights for each query term in first query\n",
      "  coronavirus: 2.0013212358202472\n",
      "  origin: 5.4422305313054355\n",
      "All queries scored in: 0m 21.88s\t\n",
      "Keyword topic queries processing complete in 2.0m 17.514423847198486s.\n",
      "\n",
      "Processing keyword topic queries ['data/hi/parsed/cord19.topics.keyword.hi2eng.gt.txt', 'term/ranked/term_rankedlist_hi2en_gt_keywords.txt']...\n",
      "TF/IDF weights for each query term in first query\n",
      "  koronavirus: 17.544534348518084\n",
      "  origin: 5.4422305313054355\n",
      "All queries scored in: 0m 11.95s\t\n",
      "Keyword topic queries processing complete in 1.0m 44.49653697013855s.\n",
      "\n",
      "Processing keyword topic queries ['data/hi/parsed/cord19.topics.keyword.hi2eng.emb.txt', 'term/ranked/term_rankedlist_hi2en_emb_keywords.txt']...\n",
      "TF/IDF weights for each query term in first query\n",
      "  coxsackievirus: 9.349777494095834\n",
      "  polerovirus: 14.959571847796926\n",
      "  erbovirus: 14.959571847796926\n",
      "  original: 6.21417763156014\n",
      "  origin: 5.4422305313054355\n",
      "All queries scored in: 0m 6.81s\t\n",
      "Keyword topic queries processing complete in 1.0m 43.586580991744995s.\n",
      "\n",
      "Processing keyword topic queries ['data/hi/parsed/cord19.topics.keyword.hi2eng.emb_synset.txt', 'term/ranked/term_rankedlist_hi2en_emb_synset_keywords.txt']...\n",
      "TF/IDF weights for each query term in first query\n",
      "  coxsackievirus: 18.699554988191668\n",
      "  polerovirus: 14.959571847796926\n",
      "  erbovirus: 14.959571847796926\n",
      "  original: 6.21417763156014\n",
      "  origin: 5.4422305313054355\n",
      "  virus: 4.183427967648194\n",
      "  hepatitis: 5.149000213055779\n",
      "  quaranjavirus: 15.22260625363072\n",
      "  corona: 6.0667760820741945\n",
      "  superbubble: 17.544534348518084\n",
      "  exclusivity: 13.02097239246107\n",
      "  transferences: 15.959571847796926\n",
      "  immigrant: 10.929824504402875\n",
      "  immigrants: 10.076928798435086\n",
      "All queries scored in: 0m 18.8s\t\n",
      "Keyword topic queries processing complete in 2.0m 27.831099033355713s.\n",
      "\n",
      "Processing keyword topic queries ['data/hi/parsed/cord19.topics.keyword.hi2eng.emb_no_oov.txt', 'term/ranked/term_rankedlist_hi2en_emb_no_oov_keywords.txt']...\n",
      "TF/IDF weights for each query term in first query\n",
      "  coxsackievirus: 9.349777494095834\n",
      "  polerovirus: 14.959571847796926\n",
      "  erbovirus: 14.959571847796926\n",
      "  original: 6.21417763156014\n",
      "  origin: 5.4422305313054355\n",
      "All queries scored in: 0m 6.77s\t\n",
      "Keyword topic queries processing complete in 1.0m 44.43154191970825s.\n",
      "\n",
      "Processing keyword topic queries ['data/hi/parsed/cord19.topics.keyword.hi2eng.gt.emb.synset.translit.txt', 'term/ranked/term_rankedlist_hi2en_gt_emb_synset_translit_keywords.txt']...\n",
      "TF/IDF weights for each query term in first query\n",
      "  koronavirus: 17.544534348518084\n",
      "  origin: 5.4422305313054355\n",
      "  coxsackievirus: 9.349777494095834\n",
      "  polerovirus: 14.959571847796926\n",
      "  erbovirus: 14.959571847796926\n",
      "  original: 6.21417763156014\n",
      "  hepatitis: 5.149000213055779\n",
      "  quaranjavirus: 15.22260625363072\n",
      "  corona: 6.0667760820741945\n",
      "  superbubble: 17.544534348518084\n",
      "  exclusivity: 13.02097239246107\n",
      "  transferences: 15.959571847796926\n",
      "  immigrant: 10.929824504402875\n",
      "  immigrants: 10.076928798435086\n",
      "All queries scored in: 0m 20.83s\t\n",
      "Keyword topic queries processing complete in 1.0m 57.76530575752258s.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for kwfile in keyword_files:\n",
    "    print(f\"Processing keyword topic queries {kwfile}...\")\n",
    "    start = time.time()\n",
    "    scores_by_qid = get_scores_by_qid(corpus, inverted_index, kwfile[0])\n",
    "    rank_top_100_scores(scores_by_qid, kwfile[1])\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    mins = elapsed // 60\n",
    "    secs = elapsed - (60*mins)\n",
    "    print(f\"Keyword topic queries processing complete in {mins}m {secs}s.\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hi2en ranked list questions for terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing question topic queries ['data/cord19.topics.question.txt', 'term/ranked/term_rankedlist_en_base_questions.txt']...\n",
      "TF/IDF weights for each query term in first query\n",
      "  origin: 5.4422305313054355\n",
      "  covid: 1.418535749448225\n",
      "  19: 1.2539034654992969\n",
      "All queries scored in: 0m 43.78s\t\n",
      "Question topic queries processing complete in 2.0m 26.91602110862732s.\n",
      "\n",
      "Processing question topic queries ['data/hi/parsed/cord19.topics.question.hi2eng.gt.txt', 'term/ranked/term_rankedlist_hi2en_gt_questions.txt']...\n",
      "TF/IDF weights for each query term in first query\n",
      "  origin: 5.4422305313054355\n",
      "All queries scored in: 0m 31.98s\t\n",
      "Question topic queries processing complete in 2.0m 14.100980997085571s.\n",
      "\n",
      "Processing question topic queries ['data/hi/parsed/cord19.topics.question.hi2eng.emb.txt', 'term/ranked/term_rankedlist_hi2en_emb_questions.txt']...\n",
      "TF/IDF weights for each query term in first query\n",
      "  origin: 5.4422305313054355\n",
      "  origins: 7.672629110858896\n",
      "  originated: 6.90338575110685\n",
      "  wondering: 13.02097239246107\n",
      "All queries scored in: 0m 31.88s\t\n",
      "Question topic queries processing complete in 2.0m 13.498536109924316s.\n",
      "\n",
      "Processing question topic queries ['data/hi/parsed/cord19.topics.question.hi2eng.emb_synset.txt', 'term/ranked/term_rankedlist_hi2en_emb_synset_questions.txt']...\n",
      "TF/IDF weights for each query term in first query\n",
      "  origin: 5.4422305313054355\n",
      "  origins: 7.672629110858896\n",
      "  originated: 6.90338575110685\n",
      "  wondering: 13.02097239246107\n",
      "All queries scored in: 0m 31.32s\t\n",
      "Question topic queries processing complete in 2.0m 13.086286067962646s.\n",
      "\n",
      "Processing question topic queries ['data/hi/parsed/cord19.topics.question.hi2eng.emb_no_oov.txt', 'term/ranked/term_rankedlist_hi2en_emb_no_oov_questions.txt']...\n",
      "TF/IDF weights for each query term in first query\n",
      "  origin: 5.4422305313054355\n",
      "  origins: 7.672629110858896\n",
      "  originated: 6.90338575110685\n",
      "  wondering: 13.02097239246107\n",
      "All queries scored in: 0m 30.41s\t\n",
      "Question topic queries processing complete in 2.0m 13.580142259597778s.\n",
      "\n",
      "Processing question topic queries ['data/hi/parsed/cord19.topics.questions.hi2eng.gt.emb.synset.translit.txt', 'term/ranked/term_rankedlist_hi2en_gt_emb_synset_translit_questions.txt']...\n",
      "TF/IDF weights for each query term in first query\n",
      "  origin: 5.4422305313054355\n",
      "  origins: 7.672629110858896\n",
      "  originated: 6.90338575110685\n",
      "  wondering: 13.02097239246107\n",
      "All queries scored in: 0m 48.47s\t\n",
      "Question topic queries processing complete in 2.0m 37.79445004463196s.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for qfile in question_files:\n",
    "    print(f\"Processing question topic queries {qfile}...\")\n",
    "    start = time.time()\n",
    "    scores_by_qid = get_scores_by_qid(corpus, inverted_index, qfile[0])\n",
    "    rank_top_100_scores(scores_by_qid, qfile[1])\n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    mins = elapsed // 60\n",
    "    secs = elapsed - (60*mins)\n",
    "    print(f\"Question topic queries processing complete in {mins}m {secs}s.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
